{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Padding': ('Padding', 0, 0.0, 0, 0.0), 'WinHttpCreateUrl': ('WinHttpCreateUrl', 1, 0.03333333333333333, 2, 6.981387620603471e-06), 'WinHttpWriteData': ('WinHttpWriteData', 2, 0.06666666666666667, 9, 3.141624429271562e-05), 'TerminateProcess': ('TerminateProcess', 3, 0.1, 112, 0.00039095770675379437), 'TerminateThread': ('TerminateThread', 4, 0.13333333333333333, 142, 0.0004956785210628465), 'CreateRemoteThread': ('CreateRemoteThread', 5, 0.16666666666666666, 199, 0.0006946480682500453), 'GetUrlCacheEntryInfo': ('GetUrlCacheEntryInfo', 6, 0.2, 311, 0.0010856057750038399), 'WinExec': ('WinExec', 7, 0.23333333333333334, 572, 0.0019966768594925927), 'HttpSendRequest': ('HttpSendRequest', 8, 0.26666666666666666, 2953, 0.010308018821821025), 'InternetOpen': ('InternetOpen', 9, 0.3, 3037, 0.01060123710188637), 'InternetConnect': ('InternetConnect', 10, 0.3333333333333333, 3274, 0.011428531534927882), 'ExitProcess': ('ExitProcess', 11, 0.36666666666666664, 3396, 0.011854396179784694), 'CreateProcess': ('CreateProcess', 12, 0.4, 4675, 0.016318993563160612), 'RegDeleteKey': ('RegDeleteKey', 13, 0.43333333333333335, 5299, 0.018497186500788895), 'CreateProcessInternal': ('CreateProcessInternal', 14, 0.4666666666666667, 8553, 0.029855904159510744), 'WinHttpReadData': ('WinHttpReadData', 15, 0.5, 9374, 0.03272176377776847), 'CreateThread': ('CreateThread', 16, 0.5333333333333333, 9694, 0.03383878579706502), 'WinHttpOpen': ('WinHttpOpen', 17, 0.5666666666666667, 9778, 0.03413200407713037), 'WinHttpOpenRequest': ('WinHttpOpenRequest', 18, 0.6, 10298, 0.03594716485848727), 'WinHttpConnect': ('WinHttpConnect', 19, 0.6333333333333333, 10310, 0.0359890531842109), 'WinHttpSendRequest': ('WinHttpSendRequest', 20, 0.6666666666666666, 10952, 0.03823007861042461), 'OpenThread': ('OpenThread', 21, 0.7, 15247, 0.05322260852567056), 'OpenProcess': ('OpenProcess', 22, 0.7333333333333333, 27048, 0.09441628618104135), 'RegSetValue': ('RegSetValue', 23, 0.7666666666666667, 111638, 0.3896940755944652), 'RegEnumValue': ('RegEnumValue', 24, 0.8, 142064, 0.49590192546670575), 'RegCreateKey': ('RegCreateKey', 25, 0.8333333333333334, 143190, 0.4998324466971055), 'LoadLibrary': ('LoadLibrary', 26, 0.8666666666666667, 263533, 0.9199130119102473), 'CopyFile': ('CopyFile', 27, 0.9, 298925, 1.0434556472444463), 'CreateFile': ('CreateFile', 28, 0.9333333333333333, 538859, 1.880991775925383), 'RegQueryValue': ('RegQueryValue', 29, 0.9666666666666667, 1075072, 3.7527471760287074), 'DeleteFile': ('DeleteFile', 30, 1.0, 1239693, 4.32738868177439)}\n",
      "C:/Users/admin/Desktop/all/allhooklogs/\n",
      "malware length minimum: 15\n",
      "name: C:/Users/admin/Desktop/all/allhooklogs/0c7183a20dc8798e815a5eea3cda860b875f636a582ae9c4110e2630cd8f18eb_3300.trace.hooklog\n",
      "---------------------------\n",
      "malware length maximum: 219743\n",
      "name: C:/Users/admin/Desktop/all/allhooklogs/ae482c29180519e6640f47347cf39ff3e5f5d071748c166de523f997afdcf4a6_3300.trace.hooklog\n",
      "---------------------------\n",
      "count： 9149\n",
      "malware length average: 426.60891900754183\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "%run C:/Users/admin/Dropbox/Code/example/Hooklog3.ipynb\n",
    "Hooklog = Hooklog3\n",
    "API_padding = False\n",
    "API_padding = True # with API padding and without API moving window\n",
    "API_moving = API_padding = True # with API padding and API moving window\n",
    "\n",
    "in_directories = [\"C:/Users/admin/Desktop/all/allhooklogs/\"]\n",
    "in_parseFirstPar = False\n",
    "test_rate = 0.1\n",
    "\n",
    "in_apifreq_dicts = [\"C:/Users/admin/Desktop/all/allhooklogs_pickletotal.pickle\"]\n",
    "\n",
    "te_dict = dict()\n",
    "\n",
    "te_dict['PAD'] = ('Padding', 0, 0.0, 0, 0.0)\n",
    "\n",
    "apifreq_dict = dict()\n",
    "_total = 0\n",
    "\n",
    "for pkf in in_apifreq_dicts:\n",
    "    with open(pkf, 'rb') as f:\n",
    "        this_dict = pickle.load(f)\n",
    "        this_dict['Padding'] = 0\n",
    "        for k in this_dict:\n",
    "            if k in apifreq_dict:\n",
    "                apifreq_dict[k] += this_dict[k]\n",
    "            else:\n",
    "                apifreq_dict[k] = this_dict[k]\n",
    "            _total += this_dict[k]\n",
    "\n",
    "te_s_dict = {item[0]: item for item in [(k, i, i/(len(apifreq_dict)-1), apifreq_dict[k], apifreq_dict[k]/286476) for i, k in enumerate(sorted(apifreq_dict, key = apifreq_dict.get, reverse=False))]}\n",
    "print(te_s_dict)\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "xx_total_list = list()\n",
    "\n",
    "xx_train_list = list()\n",
    "\n",
    "file_name = list()\n",
    "\n",
    "xx_test_list = list()\n",
    "\n",
    "train_name_list = list()\n",
    "\n",
    "minimum = 0 #最小長度\n",
    "maximum = 0 #最大長度\n",
    "total = 0 #長度總和\n",
    "count = 0\n",
    "\n",
    "name_dict = dict()\n",
    "W_Size = 128\n",
    "h_size = W_Size\n",
    "\n",
    "for label, in_dir in enumerate(in_directories):\n",
    "    print(in_dir)\n",
    "    hl_list = next(os.walk(in_dir))[2] # get all filenames in the in_directory\n",
    "    hl_list = [os.path.join(in_dir, f) for f in hl_list] # filepathname list\n",
    "    hl_list = list(filter(lambda f: f.endswith(\".hooklog\"), hl_list)) # in case some non-hooklog file in the folder\n",
    "    \n",
    "    #shuffle list\n",
    "    random.shuffle(hl_list) # <-------------- random here\n",
    "#     test_size = int(len(hl_list)*test_rate)\n",
    "\n",
    "    for i, file in enumerate(hl_list):\n",
    "        \n",
    "        file_name.append(file.split(\"/\")[-1])\n",
    "        \n",
    "        hl3 = Hooklog(file, in_parseFirstPar)\n",
    "        li_li = list() # for hacking moving_window # 20171129\n",
    "        \n",
    "        #statistic\n",
    "        if count == 0:\n",
    "            minimum = len(hl3.li)\n",
    "            \n",
    "        \n",
    "        if len(hl3.li) < minimum:\n",
    "            minimum = len(hl3.li)\n",
    "            minimum_name = file\n",
    "        \n",
    "        if len(hl3.li) > maximum:\n",
    "            maximum = len(hl3.li)\n",
    "            maximum_name = file\n",
    "            \n",
    "        total += len(hl3.li)\n",
    "        count += 1\n",
    "        \n",
    "        \n",
    "        for start in range(0, len(hl3.li), h_size):\n",
    "            end = start + h_size\n",
    "            #print(hl3.digitname, start, end)\n",
    "            \n",
    "            li = list()\n",
    "            for (t, api) in hl3.li[start:end]:\n",
    "                li.append(te_s_dict[api][4]) # <-- encode\n",
    "            # hack\n",
    "            if len(li) < h_size:\n",
    "                #print(\"!!! change img_cols to a smaller number\", len(hl3.li))\n",
    "                #print(hl3.digitname, \"has smaller size\", len(hl3.li), \"need img_cols\", str(img_cols))\n",
    "                if API_padding:\n",
    "                    #print(\"padding 1.0\", str(img_cols - len(hl3.li)))\n",
    "                    for _ in range(h_size - len(li)):\n",
    "                        li.append(0)\n",
    "            li_li.append(li)\n",
    "\n",
    "            xx_total_list.extend([li])\n",
    "            train_name_list.append((hl3.digitname, start))\n",
    "            name_dict[hl3.digitname] = label\n",
    "                \n",
    "            break; # stop moving window!! Must break!\n",
    "print (\"malware length minimum:\", minimum)\n",
    "print(\"name:\",minimum_name)\n",
    "print(\"---------------------------\")\n",
    "print (\"malware length maximum:\", maximum)\n",
    "print(\"name:\",maximum_name)\n",
    "print(\"---------------------------\")\n",
    "print(\"count：\",count)\n",
    "print (\"malware length average:\", float(total)/float(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9149\n",
      "9149\n",
      "1d3be3da74190c849fb8a618a574bb1e858df7be2ef5c0da9e401aae19a6bc85_3216.trace.hooklog\n",
      "[0.9199130119102473, 0.9199130119102473, 0.9199130119102473, 3.7527471760287074, 3.7527471760287074, 3.7527471760287074, 0.9199130119102473, 1.880991775925383, 3.7527471760287074, 3.7527471760287074, 0.49590192546670575, 0.9199130119102473, 1.880991775925383, 1.880991775925383, 4.32738868177439, 4.32738868177439, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 1.880991775925383, 0.029855904159510744, 0.029855904159510744, 0.016318993563160612, 3.7527471760287074, 0.9199130119102473, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 4.32738868177439, 0.011854396179784694, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "num = 1000\n",
    "print(len(file_name))\n",
    "print(len(xx_total_list))\n",
    "print(file_name[num])\n",
    "print(xx_total_list[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=15, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(xx_total_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9149"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "n_clusters=15\n",
    "yy_train_list = list()\n",
    "yy_test_list = list()\n",
    "train_name_list = list()\n",
    "test_name_list = list()\n",
    "\n",
    "test_size = int(len(km.labels_)*test_rate)\n",
    "\n",
    "for i in range(len(km.labels_)):\n",
    "    \n",
    "    if(i < test_size):\n",
    "        \n",
    "        xx_test_list.extend([xx_total_list[i]])\n",
    "        a = [0] * n_clusters\n",
    "        a[km.labels_[i]] = 1\n",
    "        yy_test_list.extend([a])\n",
    "        test_name_list.append(file_name[i])\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        xx_train_list.extend([xx_total_list[i]])\n",
    "        a = [0] * n_clusters\n",
    "        a[km.labels_[i]] = 1\n",
    "        yy_train_list.extend([a])\n",
    "        train_name_list.append(file_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8235, 128)\n",
      "(8235, 15)\n",
      "(914, 128)\n",
      "(914, 15)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# print(len(xx_train_list))\n",
    "# print(xx_train_list[0])\n",
    "# print(len(xx_train_list[0]))\n",
    "\n",
    "# x_train = np.ndarray(shape = (len(xx_train_list), img_rows*img_cols), buffer = np.array(xx_train_list))\n",
    "x_train = np.array(xx_train_list)\n",
    "y_train = np.array(yy_train_list)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "#x_test = x_train.copy()\n",
    "#y_test = y_train.copy()\n",
    "\n",
    "# x_test= np.ndarray(shape = (len(xx_test_list), img_rows*img_cols), buffer = np.array(xx_test_list))\n",
    "x_test = np.array(xx_test_list)\n",
    "y_test = np.array(yy_test_list)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "num_classes = n_clusters\n",
    "img_rows, img_cols = 1, 128\n",
    "filter_x, filter_y = 1, 16\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_rows*img_cols]) # None, 784\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes]) # None, 8\n",
    "\n",
    "n_hidden_1 = 128\n",
    "n_input = x_train.shape[1]\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "f_w1 = tf.get_variable('f_w1', [n_input, n_hidden_1], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "f_b1 = tf.get_variable('f_b1', [n_hidden_1], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "f_w2 = tf.get_variable('f_w2', [n_hidden_1, n_hidden_1], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "f_b2 = tf.get_variable('f_b2', [n_hidden_1], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "layer_1 = tf.add(tf.matmul(x, f_w1), f_b1)\n",
    "\n",
    "rlayer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "layer_2 = tf.add(tf.matmul(rlayer_1, f_w2), f_b2)\n",
    "\n",
    "layer_2_sigmoid = tf.sigmoid(layer_2)\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "# 小於threshold的會變成 0\n",
    "NN_threshold = tf.to_float((layer_2_sigmoid > threshold))\n",
    "\n",
    "batch_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_conv(NN_threshold, number):\n",
    "    output = tf.constant(0, tf.float32, shape=[batch_size, img_rows])\n",
    "    for i in range(number):\n",
    "        \n",
    "        if i == 0:\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            temp = NN_threshold[:, i:]\n",
    "            zero_temp = tf.constant(0, shape=[batch_size, i])\n",
    "            zero_temp = tf.cast(zero_temp, tf.float32)\n",
    "            temp_c = tf.concat([temp, zero_temp], 1)\n",
    "                        \n",
    "        output += temp_c\n",
    "    \n",
    "    NN_threshold += output\n",
    "    \n",
    "    return NN_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 4\n",
    "\n",
    "filter_addition = custom_conv(NN_threshold, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initializer(shape))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initializer(shape))\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 4, 1], strides=[1, 1, 4, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_x, filter_y = 1, 16\n",
    "img_rows, img_cols = 1, 128\n",
    "\n",
    "L1 = 64\n",
    "L2 = 128\n",
    "fc = 1024\n",
    "\n",
    "# 擷取特徵 (原始輸入 * 0 1矩陣)\n",
    "newx = tf.multiply(x, NN_threshold)\n",
    "\n",
    "# [1,16,1,64]\n",
    "W_conv1 = tf.get_variable('L_w1', [filter_x, filter_y, 1, L1], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "b_conv1 = tf.get_variable('L_b1', [L1], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "# W_conv1 = weight_variable([filter_x, filter_y, 1, L1]) # [filter_height, filter_width, in_channels, out_channels] [5, 5, 1, 32]\n",
    "# b_conv1 = bias_variable([L1]) # 32\n",
    "\n",
    "x_image = tf.reshape(newx, [-1, img_rows, img_cols, 1]) # [batch, in_height, in_width, in_channels] [-1, 28, 28, 1]\n",
    "\n",
    "output_conv1 = conv2d(x_image, W_conv1) + b_conv1 ###\n",
    "\n",
    "h_conv1 = tf.nn.relu(output_conv1)\n",
    "\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# Second Convolutional Layer\n",
    "W_conv2 = tf.get_variable('L_w2', [filter_x, filter_y, L1, L2], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "b_conv2 = tf.get_variable('L_b2', [L2], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "# W_conv2 = weight_variable([filter_x, filter_y, L1, L2]) # [5, 5, 32, 64]\n",
    "# b_conv2 = bias_variable([L2]) # 64\n",
    "\n",
    "output_conv2 = conv2d(h_pool1, W_conv2) + b_conv2 ###\n",
    "h_conv2 = tf.nn.relu(output_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# Densely Connected Layer\n",
    "s = int(img_rows*img_cols/4/4) # hack here, 2d 2x2 2 layers = 1d 1x4 2 layers\n",
    "\n",
    "W_fc1 = tf.get_variable('L_w3', [1 * s * L2, fc], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "b_fc1 = tf.get_variable('L_b3', [fc], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "# W_fc1 = weight_variable([1 * s * L2, fc]) # [7 * 7 * 64, 1024]\n",
    "# b_fc1 = bias_variable([fc]) # 1024\"\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 1*s*L2]) # [-1, 7*7*64]\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Dropout\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Readout Layer\n",
    "W_fc2 = tf.get_variable('L_w4', [fc, num_classes], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "b_fc2 = tf.get_variable('L_b4', [num_classes], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "# W_fc2 = weight_variable([fc, num_classes]) # [1024, 14]\n",
    "# b_fc2 = bias_variable([num_classes])\n",
    "\n",
    "predictions = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f_w1:0', 'f_b1:0', 'f_w2:0', 'f_b2:0']\n",
      "['L_w1:0', 'L_b1:0', 'L_w2:0', 'L_b2:0', 'L_w3:0', 'L_b3:0', 'L_w4:0', 'L_b4:0']\n"
     ]
    }
   ],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "\n",
    "# filter and Lenet\n",
    "f_vars = [var for var in tvars if 'f_' in var.name]\n",
    "L_vars = [var for var in tvars if 'L_' in var.name]\n",
    "\n",
    "print([v.name for v in f_vars])\n",
    "print([v.name for v in L_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "alfa = pow(10, -6)\n",
    "beta = 1\n",
    "gamma = 1\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = predictions, labels = y))\n",
    "\n",
    "# sum up the array and minimize this value later\n",
    "# NN_threshold\n",
    "# layer_2_sigmoid\n",
    "num_zero = tf.reduce_sum(NN_threshold)\n",
    "\n",
    "# element-wise divided by 2 and sum up\n",
    "continuous = 1 / tf.reduce_sum(tf.div(filter_addition, number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCost = alfa*cost + beta*num_zero + gamma*continuous\n",
    "\n",
    "Trainer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(newCost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_next_batch(x_test, y_test, seq, start, batch_size):\n",
    "    end = start + batch_size\n",
    "    if end > len(x_test):\n",
    "        # Shuffle the data\n",
    "        perm = np.arange(len(x_test))\n",
    "        np.random.shuffle(perm)\n",
    "        # Start next epoch\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        seq = perm\n",
    "    return x_test[seq][start:end], y_test[seq][start:end], seq, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.125 \n",
      "\n",
      "Epoch: 0001 cost = 1574.705444336 \n",
      "\n",
      "0個數的loss 1580.0   cost 2.707735   4 的個數的loss 0.0006430868 \n",
      "\n",
      "0個數的loss 1547.0   cost 1.7492164   4 的個數的loss 0.00065789477 \n",
      "\n",
      "0個數的loss 1537.0   cost 0.5668532   4 的個數的loss 0.00066236133 \n",
      "\n",
      "0個數的loss 1578.0   cost 0.7580481   4 的個數的loss 0.00064381136 \n",
      "\n",
      "step 1000, training accuracy 0.708333 \n",
      "\n",
      "Epoch: 1001 cost = 1555.879760742 \n",
      "\n",
      "0個數的loss 1582.0   cost 0.6442352   4 的個數的loss 0.0006426735 \n",
      "\n",
      "0個數的loss 1554.0   cost 0.39869976   4 的個數的loss 0.0006546645 \n",
      "\n",
      "0個數的loss 1559.0   cost 0.5127606   4 的個數的loss 0.00065220933 \n",
      "\n",
      "step 2000, training accuracy 0.958333 \n",
      "\n",
      "Epoch: 2001 cost = 1566.245727539 \n",
      "\n",
      "0個數的loss 1562.0   cost 0.31882378   4 的個數的loss 0.0006508298 \n",
      "\n",
      "0個數的loss 1548.0   cost 0.37778842   4 的個數的loss 0.00065757026 \n",
      "\n",
      "0個數的loss 1549.0   cost 0.23370945   4 的個數的loss 0.0006570302 \n",
      "\n",
      "step 3000, training accuracy 0.875 \n",
      "\n",
      "Epoch: 3001 cost = 1576.432006836 \n",
      "\n",
      "0個數的loss 1549.0   cost 0.2019672   4 的個數的loss 0.0006574622 \n",
      "\n",
      "0個數的loss 1566.0   cost 0.29332578   4 的個數的loss 0.00064966705 \n",
      "\n",
      "0個數的loss 1567.0   cost 0.28722075   4 的個數的loss 0.00064903457 \n",
      "\n",
      "0個數的loss 1558.0   cost 0.12329192   4 的個數的loss 0.00065210304 \n",
      "\n",
      "step 4000, training accuracy 0.958333 \n",
      "\n",
      "Epoch: 4001 cost = 1540.196533203 \n",
      "\n",
      "0個數的loss 1526.0   cost 0.17626415   4 的個數的loss 0.00066777965 \n",
      "\n",
      "0個數的loss 1559.0   cost 0.17677404   4 的個數的loss 0.00065231574 \n",
      "\n",
      "0個數的loss 1591.0   cost 0.037949298   4 的個數的loss 0.0006385696 \n",
      "\n",
      "step 5000, training accuracy 0.916667 \n",
      "\n",
      "Epoch: 5001 cost = 1561.273803711 \n",
      "\n",
      "0個數的loss 1546.0   cost 0.16872896   4 的個數的loss 0.00065832783 \n",
      "\n",
      "0個數的loss 1591.0   cost 0.656879   4 的個數的loss 0.00063877355 \n",
      "\n",
      "0個數的loss 1574.0   cost 0.1561168   4 的個數的loss 0.0006465169 \n",
      "\n",
      "step 6000, training accuracy 0.958333 \n",
      "\n",
      "Epoch: 6001 cost = 1557.307006836 \n",
      "\n",
      "0個數的loss 1564.0   cost 0.09004584   4 的個數的loss 0.00064966705 \n",
      "\n",
      "0個數的loss 1585.0   cost 0.04418464   4 的個數的loss 0.0006412312 \n",
      "\n",
      "0個數的loss 1520.0   cost 0.1739886   4 的個數的loss 0.0006691201 \n",
      "\n",
      "0個數的loss 1558.0   cost 0.1075829   4 的個數的loss 0.0006530612 \n",
      "\n",
      "step 7000, training accuracy 0.958333 \n",
      "\n",
      "Epoch: 7001 cost = 1566.129028320 \n",
      "\n",
      "0個數的loss 1570.0   cost 0.010296214   4 的個數的loss 0.0006487188 \n",
      "\n",
      "0個數的loss 1537.0   cost 0.1254177   4 的個數的loss 0.00066203246 \n",
      "\n",
      "0個數的loss 1529.0   cost 0.07465314   4 的個數的loss 0.0006652253 \n",
      "\n",
      "step 8000, training accuracy 0.916667 \n",
      "\n",
      "Epoch: 8001 cost = 1543.282958984 \n",
      "\n",
      "0個數的loss 1551.0   cost 0.057723116   4 的個數的loss 0.0006555228 \n",
      "\n",
      "0個數的loss 1547.0   cost 0.023223346   4 的個數的loss 0.000658003 \n",
      "\n",
      "0個數的loss 1557.0   cost 0.0836732   4 的個數的loss 0.0006538084 \n",
      "\n",
      "step 9000, training accuracy 1 \n",
      "\n",
      "Epoch: 9001 cost = 1544.017578125 \n",
      "\n",
      "0個數的loss 1584.0   cost 0.22757645   4 的個數的loss 0.0006419515 \n",
      "\n",
      "0個數的loss 1551.0   cost 0.05354369   4 的個數的loss 0.00065606035 \n",
      "\n",
      "0個數的loss 1510.0   cost 0.009090996   4 的個數的loss 0.00067487767 \n",
      "\n",
      "0個數的loss 1569.0   cost 0.010885621   4 的個數的loss 0.00064798316 \n",
      "\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "b = 0\n",
    "seq = np.arange(len(x_train))\n",
    "training_epochs = 10000\n",
    "display_step = 1000\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    batch_x, batch_y, seq, b = my_next_batch(x_train, y_train, seq, b, batch_size)\n",
    "    \n",
    "    _, c, nzero, cont = sess.run([Trainer, cost, num_zero, continuous], \n",
    "                                      feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n",
    "    \n",
    "    __ = sess.run(Trainer, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n",
    "    \n",
    "    total_cost = (c + nzero + cont)\n",
    "    \n",
    "    if epoch % 300 == 1:\n",
    "        print('0個數的loss', nzero, ' ', 'cost', c, ' ', number, '的個數的loss', cont, '\\n')\n",
    "        \n",
    "    if epoch % display_step == 0:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        \n",
    "        print(\"step %d, training accuracy %g\"%(epoch, train_accuracy), '\\n')\n",
    "        \n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost =\", \\\n",
    "            \"{:.9f}\".format(total_cost), '\\n')\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.973742\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot_layer1(layer, image, cmap = 'gray'):\n",
    "    \n",
    "    output = sess.run(layer, feed_dict = {x: image, keep_prob: 1.0})\n",
    "    \n",
    "    print(output.shape)\n",
    "    \n",
    "    count0 = 0\n",
    "    for i in range(128):\n",
    "        if output[0][i] == 0:\n",
    "            count0+=1\n",
    "    print('total ', count0, 'zeros')\n",
    "\n",
    "    img2 = output\n",
    "    img2 = img2.reshape(8,16)\n",
    "    plt.imshow(img2, interpolation='nearest', cmap=cmap)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 0. 1.]\n",
      "(1, 128)\n",
      "total  68 zeros\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reshape_all_layer = x_train[0].reshape(-1, 128)\n",
    "\n",
    "cplr = sess.run(NN_threshold, feed_dict={x : reshape_all_layer, keep_prob : 1.0})\n",
    "print(cplr[0])\n",
    "\n",
    "plot_layer1(NN_threshold, reshape_all_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
