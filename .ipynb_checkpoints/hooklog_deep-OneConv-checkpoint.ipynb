{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hooklog DEEP\n",
    "from https://www.tensorflow.org/get_started/mnist/pros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_rate = 0.1\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 1, 16*16\n",
    "filter_x, filter_y = 1, 64\n",
    "\n",
    "out_channel = 128\n",
    "\n",
    "pool_size = 4\n",
    "\n",
    "API_padding = False\n",
    "API_padding = True # with API padding and without API moving window\n",
    "API_moving = API_padding = True # with API padding and API moving window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run C:/Users/admin/Dropbox/Code/example/Hooklog3.ipynb\n",
    "Hooklog = Hooklog3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 ['allaple_woj_g_98_year2017', 'autoit_g_77', 'bettersurf_woj_g_137+', 'directdow_g_28', 'elkern_woj_g_127', 'expiro_g_56', 'fakeav_g_132', 'graftor_g_18', 'hotbar_g_32', 'kazy_g_58', 'kryptik_g_528', 'kryptik_g_529', 'kryptik_g_533', 'kryptik_g_547', 'loadmoney_g_183', 'loadmoney_g_184', 'loring_g_15', 'lydra_woj_g_44', 'mira_woj_g_107', 'morstar_g_54', 'mydoom_g_13', 'rahack_g_39', 'somoto_woj_year2017', 'symmi_g_145', 'sytro_woj_g_166', 'zbot_g_37', 'zusy_g_127', 'zusy_g_130']\n"
     ]
    }
   ],
   "source": [
    "in_directories = [\"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/allaple_woj_g_98_year2017/\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/autoit_g_77/\", \n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/bettersurf_woj_g_137+/\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/directdow_g_28/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/elkern_woj_g_127/\", #前八個\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/expiro_g_56/\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/fakeav_g_132/\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/graftor_g_18/\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/hotbar_g_32/\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/kazy_g_58/\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/kryptik_g_528/\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/kryptik_g_529/\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/kryptik_g_533/\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/kryptik_g_547/\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/loadmoney_g_183/\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/loadmoney_g_184/\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/loring_g_15/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/lydra_woj_g_44/\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/mira_woj_g_107/\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/morstar_g_54/\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/mydoom_g_13/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/rahack_g_39/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/somoto_woj_year2017/\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/symmi_g_145/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/sytro_woj_g_166/\", #前八個\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/vobfus_g_111/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/zbot_g_37/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/zusy_g_127/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/hooklogs/zusy_g_130/\"\n",
    "                 ]\n",
    "in_parseFirstPar = False\n",
    "\n",
    "in_apifreq_dicts = [\"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_allaple_woj_g_98_year2017.pickle\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_autoit_g_77.pickle\", \n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_bettersurf_woj_g_137+.pickle\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_directdow_g_28.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_elkern_woj_g_127.pickle\", #前八個\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_expiro_g_56.pickle\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_fakeav_g_132.pickle\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_graftor_g_18.pickle\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_hotbar_g_32.pickle\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_kazy_g_58.pickle\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_kryptik_g_528.pickle\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_kryptik_g_529.pickle\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_kryptik_g_533.pickle\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_kryptik_g_547.pickle\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_loadmoney_g_183.pickle\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_loadmoney_g_184.pickle\",\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_loring_g_15.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_lydra_woj_g_44.pickle\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_mira_woj_g_107.pickle\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_morstar_g_54.pickle\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_mydoom_g_13.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_rahack_g_39.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_somoto_woj_year2017.pickle\", #前八個\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_symmi_g_145.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_sytro_woj_g_166.pickle\", #前八個\n",
    "#                   \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/group_dict_vobfus.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_zbot_g_37.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_zusy_g_127.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/Tensorflow-master/pickles/apifreq_dict_zusy_g_130.pickle\"\n",
    "                   ]\n",
    "num_classes = len(in_directories)\n",
    "classnames = list(map(lambda x: x.split(\"/\")[-2], in_directories))\n",
    "print(num_classes, classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TerminateProcess': ('TerminateProcess', 0, 0.0, 2, 1.1498878859311217e-05), 'WinExec': ('WinExec', 1, 0.05555555555555555, 5, 2.8747197148278044e-05), 'InternetOpen': ('InternetOpen', 2, 0.1111111111111111, 148, 0.0008509170355890301), 'ExitProcess': ('ExitProcess', 3, 0.16666666666666666, 204, 0.0011728856436497442), 'HttpSendRequest': ('HttpSendRequest', 4, 0.2222222222222222, 254, 0.0014603576151325246), 'InternetConnect': ('InternetConnect', 5, 0.2777777777777778, 255, 0.0014661070545621802), 'CreateProcess': ('CreateProcess', 6, 0.3333333333333333, 267, 0.0015351003277180475), 'CreateThread': ('CreateThread', 7, 0.3888888888888889, 407, 0.002340021847869833), 'CreateProcessInternal': ('CreateProcessInternal', 8, 0.4444444444444444, 484, 0.0027827286839533145), 'RegDeleteKey': ('RegDeleteKey', 9, 0.5, 696, 0.004001609843040304), 'DeleteFile': ('DeleteFile', 10, 0.5555555555555556, 1203, 0.006916575633875697), 'OpenProcess': ('OpenProcess', 11, 0.6111111111111112, 1210, 0.006956821709883286), 'RegSetValue': ('RegSetValue', 12, 0.6666666666666666, 5168, 0.029713102972460183), 'RegCreateKey': ('RegCreateKey', 13, 0.7222222222222222, 5711, 0.032835048582763184), 'CopyFile': ('CopyFile', 14, 0.7777777777777778, 9393, 0.05400448456275513), 'LoadLibrary': ('LoadLibrary', 15, 0.8333333333333334, 14087, 0.08099235324555856), 'RegEnumValue': ('RegEnumValue', 16, 0.8888888888888888, 14710, 0.084574254010234), 'CreateFile': ('CreateFile', 17, 0.9444444444444444, 42800, 0.24607600758926004), 'RegQueryValue': ('RegQueryValue', 18, 1.0, 76926, 0.44228137756568736)}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "apifreq_dict = dict()\n",
    "_total = 0\n",
    "for pkf in in_apifreq_dicts:\n",
    "    with open(pkf, 'rb') as f:\n",
    "        this_dict = pickle.load(f)\n",
    "        for k in this_dict:\n",
    "            if k in apifreq_dict:\n",
    "                apifreq_dict[k] += this_dict[k]\n",
    "            else:\n",
    "                apifreq_dict[k] = this_dict[k]\n",
    "            _total += this_dict[k]\n",
    "\n",
    "s_dict = {item[0]: item for item in [(k, i, i/(len(apifreq_dict)-1), apifreq_dict[k], apifreq_dict[k]/_total) for i, k in enumerate(sorted(apifreq_dict, key = apifreq_dict.get, reverse=False))]}\n",
    "print(s_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "W_Size = 128 # moving window size\n",
    "\n",
    "xx_train_list = list()\n",
    "yy_train_list = list()\n",
    "\n",
    "xx_test_list = list()\n",
    "yy_test_list = list()\n",
    "\n",
    "train_name_list = list()\n",
    "test_name_list = list()\n",
    "\n",
    "name_dict = dict()\n",
    "count_no = 0\n",
    "for label, in_dir in enumerate(in_directories):\n",
    "    hl_list = next(os.walk(in_dir))[2] # get all filenames in the in_directory\n",
    "    hl_list = [os.path.join(in_dir, f) for f in hl_list] # filepathname list\n",
    "    hl_list = list(filter(lambda f: f.endswith(\".hooklog\"), hl_list)) # in case some non-hooklog file in the folder\n",
    "    \n",
    "    #shuffle list\n",
    "    random.shuffle(hl_list) # <-------------- random here\n",
    "    test_size = int(len(hl_list)*test_rate) # test rate = 0.1\n",
    "\n",
    "    \n",
    "    for i, file in enumerate(hl_list):\n",
    "        hl3 = Hooklog(file, in_parseFirstPar)\n",
    "        li_li = list() # for hacking moving_window # 20171129\n",
    "        \n",
    "        count_no += 1\n",
    "        \n",
    "        for start in range(0, len(hl3.li), W_Size): # img_cols = 128 0 to length of hooklog\n",
    "            end = start + img_cols\n",
    "#             print(hl3.digitname, start, end)\n",
    "            \n",
    "            li = list()\n",
    "            for (t, api) in hl3.li[start:end]: # moving\n",
    "                li.append(s_dict[api][4]) # <-- encode\n",
    "            \n",
    "            # hack\n",
    "            if len(li) < img_cols:\n",
    "                #print(\"!!! change img_cols to a smaller number\", len(hl3.li))\n",
    "                #print(hl3.digitname, \"has smaller size\", len(hl3.li), \"need img_cols\", str(img_cols))\n",
    "                if API_padding:\n",
    "                    #print(\"padding 1.0\", str(img_cols - len(hl3.li)))\n",
    "                    for _ in range(img_cols - len(li)):\n",
    "                        li.append(0)\n",
    "                        #li.append(1.0)\n",
    "            li_li.append(li)\n",
    "            \n",
    "#             print('start')\n",
    "#             print(count_no)\n",
    "#             print(len(li))\n",
    "#             print(li)\n",
    "#             count_no += 1\n",
    "#             print('end')\n",
    "            \n",
    "            if(i < test_size):\n",
    "                xx_test_list.extend([li])\n",
    "                a = [0] * num_classes\n",
    "                a[label] = 1\n",
    "                yy_test_list.extend([a])\n",
    "                test_name_list.append((hl3.digitname, start))\n",
    "                name_dict[hl3.digitname] = label\n",
    "            else:\n",
    "                xx_train_list.extend([li])\n",
    "                a = [0] * num_classes\n",
    "                a[label] = 1\n",
    "                yy_train_list.extend([a])\n",
    "                train_name_list.append((hl3.digitname, start))\n",
    "                name_dict[hl3.digitname] = label\n",
    "\n",
    "print(count_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(654, 256)\n",
      "(654, 16)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#x_train = np.ndarray(shape = (len(xx_train_list), img_rows, img_cols), buffer = np.array(xx_train_list))\n",
    "#y_train = np.array(yy_train_list)\n",
    "x_train = np.ndarray(shape = (len(xx_train_list), img_rows*img_cols), buffer = np.array(xx_train_list))\n",
    "y_train = np.array(yy_train_list)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_test = x_train.copy()\n",
    "#y_test = y_train.copy()\n",
    "\n",
    "x_test= np.ndarray(shape = (len(xx_test_list), img_rows*img_cols), buffer = np.array(xx_test_list))\n",
    "y_test = np.array(yy_test_list)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(train_name_list)\n",
    "#print(test_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_rows*img_cols]) # None, 784\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, num_classes]) # None, 8\n",
    "# None indicates that the first dimension, corresponding to the batch size, can be of any size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer\n",
    "https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_convolutional_neural_networks_work.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') # \"SAME\" tries to pad evenly left and right\n",
    "    #The stride of the sliding window for each dimension of input x. \n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, pool_size, 1], strides=[1, 1, 4, 1], padding='SAME') # [1, 2, 2, 1], [1, 2, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First Convolutional Layer\n",
    "\n",
    "# [1,4,1,32]\n",
    "W_conv1 = weight_variable([filter_x, filter_y, 1, out_channel]) # [filter_height, filter_width, in_channels, out_channels] [5, 5, 1, 32]\n",
    "b_conv1 = bias_variable([out_channel]) # 32\n",
    "\n",
    "x_image = tf.reshape(x, [-1, img_rows, img_cols, 1]) # [batch, in_height, in_width, in_channels] [-1, 28, 28, 1]\n",
    "\n",
    "output_conv1 = conv2d(x_image, W_conv1) + b_conv1 ###\n",
    "h_conv1 = tf.nn.relu(output_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Second Convolutional Layer\n",
    "# W_conv2 = weight_variable([filter_x, filter_y, 32, 64]) # [5, 5, 32, 64]\n",
    "# b_conv2 = bias_variable([64]) # 64\n",
    "\n",
    "# output_conv2 = conv2d(h_pool1, W_conv2) + b_conv2 ###\n",
    "# h_conv2 = tf.nn.relu(output_conv2)\n",
    "# h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Densely Connected Layer\n",
    "s = int(img_rows*img_cols/4) # hack here, 2d 2x2 2 layers = 1d 1x4 2 layers\n",
    "\n",
    "W_fc1 = weight_variable([1 * s * out_channel, out_channel*2]) # [7 * 7 * 64, 1024]\n",
    "b_fc1 = bias_variable([out_channel*2]) # 1024\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool1, [-1, 1*s*out_channel]) # [-1, 7*7*64] ###\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Readout Layer\n",
    "W_fc2 = weight_variable([out_channel*2, num_classes]) # [1024, 8]\n",
    "b_fc2 = bias_variable([num_classes]) # 8\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and Evaluate \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 1000 86 50\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "training_epochs = 1000\n",
    "testing_size = len(x_test)\n",
    "print_interval = 50\n",
    "\n",
    "print(batch_size, training_epochs, testing_size, print_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# use this one to keep the session\\nsess = tf.InteractiveSession()\\nsess.run(tf.global_variables_initializer())\\nfor i in range(training_epochs):\\n    batch = mnist.train.next_batch(50)\\n    if i%print_interval == 0:\\n        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\\n        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\\n    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\\n'"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# use this one to keep the session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(training_epochs):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%print_interval == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_next_batch(x_test, y_test, seq, start, batch_size):\n",
    "    end = start + batch_size\n",
    "    if end > len(x_test):\n",
    "        # Shuffle the data\n",
    "        perm = np.arange(len(x_test))\n",
    "        np.random.shuffle(perm)\n",
    "        # Start next epoch\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        seq = perm\n",
    "    return x_test[seq][start:end], y_test[seq][start:end], seq, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0\n",
      "step 50, training accuracy 0\n",
      "step 100, training accuracy 0.45\n",
      "step 150, training accuracy 0.45\n",
      "step 200, training accuracy 0.75\n",
      "step 250, training accuracy 0.8\n",
      "step 300, training accuracy 0.75\n",
      "step 350, training accuracy 0.75\n",
      "step 400, training accuracy 0.75\n",
      "step 450, training accuracy 1\n",
      "step 500, training accuracy 0.75\n",
      "step 550, training accuracy 0.85\n",
      "step 600, training accuracy 0.75\n",
      "step 650, training accuracy 0.8\n",
      "step 700, training accuracy 0.9\n",
      "step 750, training accuracy 0.85\n",
      "step 800, training accuracy 0.95\n",
      "step 850, training accuracy 0.8\n",
      "step 900, training accuracy 0.9\n",
      "step 950, training accuracy 1\n",
      "Cost 22.137782 sec\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "import time\n",
    "tStart = time.time()\n",
    "\n",
    "b = 0\n",
    "seq = np.arange(len(x_train))\n",
    "for i in range(training_epochs):\n",
    "    batch_x, batch_y, seq, b = my_next_batch(x_train, y_train, seq, b, batch_size)\n",
    "    \n",
    "    if i%print_interval == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch_x, y_: batch_y, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.5})\n",
    "\n",
    "tEnd = time.time()\n",
    "tTime = tEnd - tStart\n",
    "print(\"Cost %f\" %(tTime), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.918605\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: x_test[:testing_size], y_: y_test[:testing_size], keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def plot_layer(layer, image):\n",
    "#     # layer 輸入範例是 [batch_num, width, height, channels] ex. [1, 28, 28, 32]\n",
    "#     # 這裡設定的 image 是單一個手寫字影像輸入\n",
    "#     # 接下來把它餵入 tensorflow 的 session 跑出我們想要的輸出結果\n",
    "# #     sess = tf.InteractiveSession()\n",
    "# #     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     output = sess.run(layer, feed_dict = {x: [image]}) # 執行 Convolution / Pooling\n",
    "    \n",
    "#     print('output.shape = ' + str(output.shape))\n",
    "#     print(type(output))\n",
    "#     print(output[0,0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def plot_fc_layer(layer, image):\n",
    "# #     sess = tf.InteractiveSession()\n",
    "# #     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     output = sess.run(layer, feed_dict = {x: [image]})\n",
    "    \n",
    "#     fc = tf.nn.relu(tf.matmul(output, W_fc1) + b_fc1)\n",
    "    \n",
    "#     sfc = sess.run(tf.reshape(fc,[1024]))\n",
    "    \n",
    "#     print('output.shape = ' + str(fc.shape))\n",
    "#     print(type(fc))\n",
    "#     print(sfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def plot_ro_layer(layer, image):\n",
    "# #     sess = tf.InteractiveSession()\n",
    "# #     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     output = sess.run(layer, feed_dict = {x: [image]})\n",
    "    \n",
    "# #     h_fc1 = tf.nn.relu(tf.matmul(output, W_fc1) + b_fc1)\n",
    "    \n",
    "# #     keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "#     h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "#     y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "#     print('output.shape = ' + str(y_conv.shape))\n",
    "#     print(type(y_conv))\n",
    "#     print(sess.run(y_conv, feed_dict = {x: [image], keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(batch_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(batch_x[0]) # 128\n",
    "# print(batch_x[0].shape)\n",
    "# print(type(batch_x[0]))\n",
    "# print(batch_y[0]) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c_in = [0.07609941330414108 for i in range(64)]\n",
    "# c_input = np.asarray(c_in)\n",
    "# print(c_input)\n",
    "# print(c_input.shape)\n",
    "# print(type(c_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output_conv1 = conv2d(x_image, W_conv1) + b_conv1\n",
    "# image1 = batch_x[0] # input dat\n",
    "# image1 = c_input\n",
    "# print(image1)\n",
    "# plot_layer(h_conv1, image1)\n",
    "\n",
    "# print(type(h_conv1))\n",
    "\n",
    "# print(h_conv1[0,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pool1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# h_pool1 = max_pool_2x2(h_conv1)\n",
    "# plot_layer(h_pool1, image1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight\n",
    "## conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(W_conv1)\n",
    "# a = sess.run(W_conv1)\n",
    "# for i in range(32):\n",
    "#     print(a[0,:,0,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(b_conv1)\n",
    "# print(b_conv1.shape)\n",
    "# a = sess.run(b_conv1)\n",
    "# for i in range(32):\n",
    "#     print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TensorFlow_GPU]",
   "language": "python",
   "name": "conda-env-TensorFlow_GPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
