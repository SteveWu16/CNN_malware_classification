{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets('C:/test/tensorflow/mnist/input_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unrolled through 28 time steps\n",
    "time_steps = 64\n",
    "#hidden LSTM units\n",
    "num_units = 128\n",
    "#rows of 28 pixels\n",
    "n_input = 64\n",
    "#learning rate for adam\n",
    "learning_rate = 0.001\n",
    "#mnist is meant to be classified in 10 classes(0-9).\n",
    "#n_classes=10\n",
    "#size of batch\n",
    "batch_size = 16\n",
    "num_layers = 2\n",
    "\n",
    "test_rate = 0.1\n",
    "h_size = time_steps * n_input\n",
    "API_padding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%run ../MotifAnalysis/Hooklog3.ipynb\n",
    "%run Hooklog3.ipynb\n",
    "Hooklog = Hooklog3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# old data\n",
    "'''\n",
    "in_base_dir = \"C:/test/tensorflow/hooklog/\"\n",
    "in_directories = [\"allaple_woj_g_98_year2017\", # 16k\n",
    "                  \"somoto_woj_year2017\", # 17k\n",
    "                  \"morstar_g_54\", # 85k\n",
    "                  \"bettersurf_woj_g_137+\", # 106k\n",
    "                  \"elkern_woj_g_127\", # 44k\n",
    "                  \"lydra_woj_g_44\", # 30k\n",
    "                  \"mira_woj_g_107\", # 14k\n",
    "                  \"sytro_woj_g_166\" # 34k\n",
    "                 ]\n",
    "\n",
    "# new data\n",
    "in_base_dir = \"C:/Users/hsiao/Downloads/GitHub/Tensorflow/hooklogs/\"\n",
    "in_directories = [\n",
    "    \"allaple_woj_g_98_year2017\",\n",
    "    \"bettersurf_woj_g_137+\",\n",
    "    \"elkern_woj_g_127\",\n",
    "    \"fakeav_g_132\",\n",
    "    \"loring_g_15\",\n",
    "    \"morstar_g_54\",\n",
    "    \"rahack_g_39\",\n",
    "    \"sytro_woj_g_166\"\n",
    "]\n",
    "'''\n",
    "\n",
    "# yj\n",
    "#in_base_dir = \"C:/test/hooklog_family_yj/hooklog/\"\n",
    "in_base_dir = \"T:/ftp/hooklog/allhooklog/family/\"\n",
    "in_directories = None\n",
    "if in_directories is None:\n",
    "    import os\n",
    "    in_directories = next(os.walk(in_base_dir))[1]\n",
    "\n",
    "# real dirs\n",
    "in_directories = [in_base_dir + d + '/' for d in in_directories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 ['allaple_woj_g_98_year2017', 'bettersurf_woj_g_137+', 'elkern_woj_g_127', 'graftor_g_18', 'hotbar_g_32', 'kryptik_g_529', 'kryptik_g_547', 'loadmoney_g_183', 'loring_g_15', 'mydoom_g_13', 'rahack_g_39', 'sytro_woj_g_166', 'vobfus_g_111', 'zbot_g_37']\n"
     ]
    }
   ],
   "source": [
    "'''in_parseFirstPar = False\n",
    "\n",
    "# hooklog's freq\n",
    "#in_apifreq_dir = \"C:/Users/hsiao/Dropbox/notebook/MotifAnalysis/pickle/\"\n",
    "#in_apifreq_dir = \"C:/Users/hsiao/Downloads/GitHub/Tensorflow/pickles/\"\n",
    "#in_apifreq_dir = \"C:/test/hooklog_family_yj/pickle/\"\n",
    "\n",
    "#syscall's freq\n",
    "if ishooklog: # Windows API\n",
    "    #in_apifreq_dir = \"T:/ftp/Systemcall/hooklog_family/pickle/\"\n",
    "    in_apifreq_dir = \"T:/ftp/Tensorflow-master/pickles_goodfamily/\"\n",
    "elif isstrace: # syscall\n",
    "    in_apifreq_dir = \"T:/ftp/Systemcall/malware_syscall/pickle/\"\n",
    "else: # som 要改\n",
    "    in_apifreq_dir = \"T:/ftp/Systemcall/malware_syscall/pickle/\" \n",
    "\n",
    "#in_apifreq_dir = \"T:/ftp/Systemcall/hooklog_family/pickle/\"\n",
    "\n",
    "in_apifreq_dicts = [in_apifreq_dir + apifreq_dict_ + d.split(\"/\")[-2] + \".pickle\" for d in in_directories]\n",
    "\n",
    "# info\n",
    "n_classes = len(in_directories)\n",
    "classnames = list(map(lambda x: x.split(\"/\")[-2], in_directories))\n",
    "print(n_classes, classnames)'''\n",
    "\n",
    "in_parseFirstPar = False\n",
    "\n",
    "#in_apifreq_dir = \"C:/Users/hsiao/Dropbox/notebook/MotifAnalysis/pickle/\"\n",
    "#in_apifreq_dir = \"C:/Users/hsiao/Downloads/GitHub/Tensorflow/pickles/\"\n",
    "in_apifreq_dir = \"T:/ftp/hooklog/allhooklog/pickle/\"\n",
    "\n",
    "in_apifreq_dicts = [in_apifreq_dir+\"apifreq_dict_\"+d.split(\"/\")[-2]+\".pickle\" for d in in_directories]\n",
    "\n",
    "n_classes = len(in_directories)\n",
    "classnames = list(map(lambda x: x.split(\"/\")[-2], in_directories))\n",
    "print(n_classes, classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreateRemoteThread': ('CreateRemoteThread', 0, 0.0, 3, 3.896038118836955e-06), 'OpenThread': ('OpenThread', 1, 0.04, 14, 1.8181511221239123e-05), 'WinExec': ('WinExec', 2, 0.08, 15, 1.9480190594184772e-05), 'TerminateProcess': ('TerminateProcess', 3, 0.12, 21, 2.7272266831858684e-05), 'GetUrlCacheEntryInfo': ('GetUrlCacheEntryInfo', 4, 0.16, 33, 4.28564193072065e-05), 'WinHttpOpen': ('WinHttpOpen', 5, 0.2, 75, 9.740095297092386e-05), 'WinHttpConnect': ('WinHttpConnect', 6, 0.24, 75, 9.740095297092386e-05), 'WinHttpOpenRequest': ('WinHttpOpenRequest', 7, 0.28, 75, 9.740095297092386e-05), 'WinHttpSendRequest': ('WinHttpSendRequest', 8, 0.32, 170, 0.00022077549340076077), 'CreateProcess': ('CreateProcess', 9, 0.36, 426, 0.0005532374128748475), 'ExitProcess': ('ExitProcess', 10, 0.4, 618, 0.0008025838524804127), 'InternetOpen': ('InternetOpen', 11, 0.44, 779, 0.0010116712315246625), 'CreateProcessInternal': ('CreateProcessInternal', 12, 0.48, 791, 0.0010272553840000103), 'HttpSendRequest': ('HttpSendRequest', 13, 0.52, 856, 0.0011116695432414777), 'InternetConnect': ('InternetConnect', 14, 0.56, 912, 0.0011843955881264343), 'RegDeleteKey': ('RegDeleteKey', 15, 0.6, 1856, 0.0024103489161871293), 'OpenProcess': ('OpenProcess', 16, 0.64, 2019, 0.0026220336539772705), 'CreateThread': ('CreateThread', 17, 0.68, 3927, 0.005099913897557574), 'RegSetValue': ('RegSetValue', 18, 0.72, 16433, 0.021341198135615892), 'RegCreateKey': ('RegCreateKey', 19, 0.76, 18481, 0.024000893491408586), 'CopyFile': ('CopyFile', 20, 0.8, 27247, 0.035385116874650166), 'RegEnumValue': ('RegEnumValue', 21, 0.84, 51961, 0.067480678897629), 'LoadLibrary': ('LoadLibrary', 22, 0.88, 52652, 0.06837806634433445), 'CreateFile': ('CreateFile', 23, 0.92, 143427, 0.18626568642347596), 'DeleteFile': ('DeleteFile', 24, 0.96, 160671, 0.20866011353055078), 'RegQueryValue': ('RegQueryValue', 25, 1.0, 286476, 0.3720404720439785)}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "apifreq_dict = dict()\n",
    "_total = 0\n",
    "for pkf in in_apifreq_dicts:\n",
    "    with open(pkf, 'rb') as f:\n",
    "        this_dict = pickle.load(f)\n",
    "        for k in this_dict:\n",
    "            if k in apifreq_dict:\n",
    "                apifreq_dict[k] += this_dict[k]\n",
    "            else:\n",
    "                apifreq_dict[k] = this_dict[k]\n",
    "            _total += this_dict[k]\n",
    "\n",
    "s_dict = {item[0]: item for item in [(k, i, i/(len(apifreq_dict)-1), apifreq_dict[k], apifreq_dict[k]/_total) for i, k in enumerate(sorted(apifreq_dict, key = apifreq_dict.get, reverse=False))]}\n",
    "print(s_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malware length minimum: 18\n",
      "name: T:/ftp/hooklog/allhooklog/family/vobfus_g_111/1d3cfb9bd2754df6db4994e07667c46a2b0c1f0eeee90511afe518c598c73dcf_3200.trace.hooklog\n",
      "---------------------------\n",
      "malware length maximum: 20766\n",
      "name: T:/ftp/hooklog/allhooklog/family/loadmoney_g_183/ffe3854f6b135356e619680964df8790_3220.trace.hooklog\n",
      "---------------------------\n",
      "count： 1940\n",
      "malware length average: 396.9139175257732\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "xx_train_list = list()\n",
    "yy_train_list = list()\n",
    "\n",
    "xx_test_list = list()\n",
    "yy_test_list = list()\n",
    "\n",
    "train_name_list = list()\n",
    "test_name_list = list()\n",
    "\n",
    "minimum = 0 #最小長度\n",
    "maximum = 0 #最大長度\n",
    "total = 0 #長度總和\n",
    "count = 0\n",
    "\n",
    "name_dict = dict()\n",
    "\n",
    "for label, in_dir in enumerate(in_directories):\n",
    "    hl_list = next(os.walk(in_dir))[2] # get all filenames in the in_directory\n",
    "    hl_list = [os.path.join(in_dir, f) for f in hl_list] # filepathname list\n",
    "    hl_list = list(filter(lambda f: f.endswith(\".hooklog\"), hl_list)) # in case some non-hooklog file in the folder\n",
    "    \n",
    "    #shuffle list\n",
    "    random.shuffle(hl_list) # <-------------- random here\n",
    "    test_size = int(len(hl_list)*test_rate)\n",
    "\n",
    "    for i, file in enumerate(hl_list):\n",
    "        hl3 = Hooklog(file, in_parseFirstPar)\n",
    "        li_li = list() # for hacking moving_window # 20171129\n",
    "        \n",
    "        #statistic\n",
    "        if count == 0:\n",
    "            minimum = len(hl3.li)\n",
    "            \n",
    "        \n",
    "        if len(hl3.li) < minimum:\n",
    "            minimum = len(hl3.li)\n",
    "            minimum_name = file\n",
    "        \n",
    "        if len(hl3.li) > maximum:\n",
    "            maximum = len(hl3.li)\n",
    "            maximum_name = file\n",
    "            \n",
    "        total += len(hl3.li)\n",
    "        count += 1\n",
    "        \n",
    "        \n",
    "        for start in range(0, len(hl3.li), h_size):\n",
    "            end = start + h_size\n",
    "            #print(hl3.digitname, start, end)\n",
    "            \n",
    "            li = list()\n",
    "            for (t, api) in hl3.li[start:end]:\n",
    "                li.append(s_dict[api][4]) # <-- encode\n",
    "            # hack\n",
    "            if len(li) < h_size:\n",
    "                #print(\"!!! change img_cols to a smaller number\", len(hl3.li))\n",
    "                #print(hl3.digitname, \"has smaller size\", len(hl3.li), \"need img_cols\", str(img_cols))\n",
    "                if API_padding:\n",
    "                    #print(\"padding 1.0\", str(img_cols - len(hl3.li)))\n",
    "                    for _ in range(h_size - len(li)):\n",
    "                        li.append(1.0)\n",
    "            li_li.append(li)\n",
    "\n",
    "            if(i < test_size):\n",
    "                xx_test_list.extend([li])\n",
    "                a = [0] * n_classes\n",
    "                a[label] = 1\n",
    "                yy_test_list.extend([a])\n",
    "                test_name_list.append((hl3.digitname, start))\n",
    "                name_dict[hl3.digitname] = label\n",
    "            else:\n",
    "                xx_train_list.extend([li])\n",
    "                a = [0] * n_classes\n",
    "                a[label] = 1\n",
    "                yy_train_list.extend([a])\n",
    "                train_name_list.append((hl3.digitname, start))\n",
    "                name_dict[hl3.digitname] = label\n",
    "                \n",
    "            break; # stop moving window!! Must break!\n",
    "print (\"malware length minimum:\", minimum)\n",
    "print(\"name:\",minimum_name)\n",
    "print(\"---------------------------\")\n",
    "print (\"malware length maximum:\", maximum)\n",
    "print(\"name:\",maximum_name)\n",
    "print(\"---------------------------\")\n",
    "print(\"count：\",count)\n",
    "print (\"malware length average:\", float(total)/float(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1751, 4096)\n",
      "(1751, 14)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#x_train = np.ndarray(shape = (len(xx_train_list), img_rows, img_cols), buffer = np.array(xx_train_list))\n",
    "#y_train = np.array(yy_train_list)\n",
    "x_train = np.ndarray(shape = (len(xx_train_list), time_steps * n_input), buffer = np.array(xx_train_list))\n",
    "y_train = np.array(yy_train_list)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 4096)\n",
      "(189, 14)\n"
     ]
    }
   ],
   "source": [
    "#x_test = x_train.copy()\n",
    "#y_test = y_train.copy()\n",
    "\n",
    "x_test= np.ndarray(shape = (len(xx_test_list), time_steps * n_input), buffer = np.array(xx_test_list))\n",
    "y_test = np.array(yy_test_list)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#weights and biases of appropriate shape to accomplish above task\n",
    "out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n",
    "out_bias=tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input image placeholder\n",
    "x=tf.placeholder(\"float\",[None,time_steps,n_input])\n",
    "#input label placeholder\n",
    "y=tf.placeholder(\"float\",[None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input_x=tf.unstack(x, time_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_a_cell(lstm_size):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1, state_is_tuple=True)\n",
    "    #drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=1.0) #output_keep_prob設為1代表不會dropout\n",
    "    #return drop\n",
    "    return lstm\n",
    "\n",
    "lstm_layer=tf.contrib.rnn.MultiRNNCell([get_a_cell(num_units) for _ in range(num_layers)], state_is_tuple=True) #設定LSTM層數\n",
    "outputs,_=tf.nn.static_rnn(lstm_layer,input_x,dtype=\"float32\") # Mike: I change it to tf.nn.\n",
    "    \n",
    "    #converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "    \n",
    "    \n",
    "    #loss_function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "    #optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    #model evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_next_batch(x_test, y_test, seq, start, batch_size):\n",
    "    end = start + batch_size\n",
    "    if end > len(x_test):\n",
    "        # Shuffle the data\n",
    "        perm = np.arange(len(x_test))\n",
    "        np.random.shuffle(perm)\n",
    "        # Start next epoch\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        seq = perm\n",
    "    return x_test[seq][start:end], y_test[seq][start:end], seq, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch: 10 Accuracy: 1.0 Loss: 0.0\n",
      "For epoch: 20 Accuracy: 1.0 Loss: 0.0\n",
      "For epoch: 30 Accuracy: 1.0 Loss: 0.0\n",
      "For epoch: 40 Accuracy: 0.0 Loss: 4.94511\n",
      "For epoch: 50 Accuracy: 1.0 Loss: 3.29012e-05\n",
      "For epoch: 60 Accuracy: 0.0 Loss: 28.613\n",
      "For epoch: 70 Accuracy: 0.0 Loss: 4.5268\n",
      "For epoch: 80 Accuracy: 1.0 Loss: 1.3113e-06\n",
      "For epoch: 90 Accuracy: 0.0 Loss: 5.37496\n",
      "For epoch: 100 Accuracy: 1.0 Loss: 0.0119684\n",
      "For epoch: 110 Accuracy: 0.125 Loss: 11.4047\n",
      "For epoch: 120 Accuracy: 0.0625 Loss: 6.04169\n",
      "For epoch: 130 Accuracy: 0.0 Loss: 5.17686\n",
      "For epoch: 140 Accuracy: 0.0 Loss: 3.96797\n",
      "For epoch: 150 Accuracy: 0.25 Loss: 2.46588\n",
      "For epoch: 160 Accuracy: 0.3125 Loss: 1.91048\n",
      "For epoch: 170 Accuracy: 0.4375 Loss: 2.2453\n",
      "For epoch: 180 Accuracy: 0.1875 Loss: 2.85926\n",
      "For epoch: 190 Accuracy: 0.5625 Loss: 1.82036\n",
      "For epoch: 200 Accuracy: 0.25 Loss: 1.98519\n",
      "For epoch: 210 Accuracy: 0.4375 Loss: 1.92382\n",
      "For epoch: 220 Accuracy: 0.4375 Loss: 2.04085\n",
      "For epoch: 230 Accuracy: 0.125 Loss: 2.43306\n",
      "For epoch: 240 Accuracy: 0.4375 Loss: 1.7854\n",
      "For epoch: 250 Accuracy: 0.4375 Loss: 1.86778\n",
      "For epoch: 260 Accuracy: 0.25 Loss: 2.66557\n",
      "For epoch: 270 Accuracy: 0.375 Loss: 2.05739\n",
      "For epoch: 280 Accuracy: 0.25 Loss: 2.4208\n",
      "For epoch: 290 Accuracy: 0.1875 Loss: 2.10448\n",
      "For epoch: 300 Accuracy: 0.1875 Loss: 2.44804\n",
      "For epoch: 310 Accuracy: 0.1875 Loss: 2.36151\n",
      "For epoch: 320 Accuracy: 0.3125 Loss: 2.25726\n",
      "For epoch: 330 Accuracy: 0.3125 Loss: 2.04544\n",
      "For epoch: 340 Accuracy: 0.4375 Loss: 1.86958\n",
      "For epoch: 350 Accuracy: 0.25 Loss: 2.66317\n",
      "For epoch: 360 Accuracy: 0.375 Loss: 2.29216\n",
      "For epoch: 370 Accuracy: 0.375 Loss: 1.9125\n",
      "For epoch: 380 Accuracy: 0.5 Loss: 1.78954\n",
      "For epoch: 390 Accuracy: 0.5 Loss: 1.84878\n",
      "For epoch: 400 Accuracy: 0.25 Loss: 2.13451\n",
      "For epoch: 410 Accuracy: 0.3125 Loss: 2.13985\n",
      "For epoch: 420 Accuracy: 0.25 Loss: 2.43056\n",
      "For epoch: 430 Accuracy: 0.1875 Loss: 2.35181\n",
      "For epoch: 440 Accuracy: 0.25 Loss: 2.2209\n",
      "For epoch: 450 Accuracy: 0.3125 Loss: 1.85804\n",
      "For epoch: 460 Accuracy: 0.25 Loss: 2.48607\n",
      "For epoch: 470 Accuracy: 0.4375 Loss: 2.3726\n",
      "For epoch: 480 Accuracy: 0.375 Loss: 2.01791\n",
      "For epoch: 490 Accuracy: 0.25 Loss: 2.02128\n",
      "total time: 31.785465002059937\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "epoch = 1\n",
    "\n",
    "b = 0\n",
    "seq = np.arange(len(x_train))\n",
    "t0 = time.time()\n",
    "\n",
    "while epoch < 500:\n",
    "    #batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\n",
    "    batch_x, batch_y, seq, b = my_next_batch(x_train, y_train, seq, b, batch_size)\n",
    "    batch_x = batch_x.reshape((batch_size, time_steps, n_input))\n",
    "    #print(type(batch_x), batch_x.shape, type(batch_y), batch_y.shape)\n",
    "\n",
    "    sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "    if epoch %10 == 0:\n",
    "        acc = sess.run(accuracy, feed_dict = {x: batch_x, y: batch_y})\n",
    "        los = sess.run(loss, feed_dict = {x: batch_x, y: batch_y})\n",
    "        print(\"For epoch:\", epoch, \"Accuracy:\", acc, \"Loss:\",los)\n",
    "        #if acc >= 0.95: break # hack\n",
    "\n",
    "    epoch += 1\n",
    "    \n",
    "t1 = time.time()\n",
    "\n",
    "print(\"total time:\", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.338624\n"
     ]
    }
   ],
   "source": [
    "#calculating test accuracy\n",
    "#test_data = mnist.test.images[:128].reshape((-1, time_steps, n_input))\n",
    "#test_label = mnist.test.labels[:128]\n",
    "test_data = x_test.reshape((-1, time_steps, n_input))\n",
    "test_label = y_test\n",
    "print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict = {x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 train_able_variables in the Graph\n",
      "<tf.Variable 'Variable:0' shape=(128, 14) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=(14,) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(192, 512) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "vs = tf.trainable_variables()\n",
    "print('There are', len(vs), 'train_able_variables in the Graph')\n",
    "for v in vs:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some other data ##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# the image\\nwhich = 0\\n#first_array = x_test[which].reshape(8, int(num_units/8))\\nfirst_array = x_test[which].reshape(8, 8)\\n#print(test_name_list[which])\\n#print(x_test[which])\\nplt.imshow(first_array) # color viridis\\nplt.matshow(first_array, cmap = plt.get_cmap(\\'gray\\'))\\n\\nplt.show()\\nplt.tight_layout()\\nplt.savefig(\"T:/visualize.pdf\", dpi=300)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# the image\n",
    "which = 0\n",
    "#first_array = x_test[which].reshape(8, int(num_units/8))\n",
    "first_array = x_test[which].reshape(8, 8)\n",
    "#print(test_name_list[which])\n",
    "#print(x_test[which])\n",
    "plt.imshow(first_array) # color viridis\n",
    "plt.matshow(first_array, cmap = plt.get_cmap('gray'))\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"T:/visualize.pdf\", dpi=300)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ns_x, s_y = len(x_test), 1\\nf, axarr = plt.subplots(n_classes, s_y, figsize=(10,n_classes))\\nprv_classname = None\\nidx = 0\\nfor i in range(s_x):\\n    for j in range(s_y):\\n            \\n        if s_y != 1:\\n            axarr[i,j].imshow(x_test[i+j].reshape(1,time_steps*n_input))\\n            name, start = test_name_list[i+j]\\n            axarr[i,j].set_title(classnames[name_dict[name]]+ \":\" + name )\\n        else:\\n            name, start = test_name_list[i]\\n            if prv_classname == classnames[name_dict[name]]:\\n                continue\\n            \\n            axarr[idx].imshow(x_test[i+j].reshape(1,time_steps*n_input))\\n            #name, start = test_name_list[i]\\n            #axarr[idx].set_title(classnames[name_dict[name]]+ \":\" + name )\\n            axarr[idx].set_title(classnames[name_dict[name]].split(\\'_\\')[0]+ \":\" + name )\\n            axarr[idx].axis(\\'off\\')\\n            idx += 1\\n            #print(idx)\\n        prv_classname = classnames[name_dict[name]]\\n\\nplt.tight_layout()\\nplt.savefig(\"T:/APIEncoding.pdf\", dpi=300)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(n_classes)\n",
    "'''\n",
    "s_x, s_y = len(x_test), 1\n",
    "f, axarr = plt.subplots(n_classes, s_y, figsize=(10,n_classes))\n",
    "prv_classname = None\n",
    "idx = 0\n",
    "for i in range(s_x):\n",
    "    for j in range(s_y):\n",
    "            \n",
    "        if s_y != 1:\n",
    "            axarr[i,j].imshow(x_test[i+j].reshape(1,time_steps*n_input))\n",
    "            name, start = test_name_list[i+j]\n",
    "            axarr[i,j].set_title(classnames[name_dict[name]]+ \":\" + name )\n",
    "        else:\n",
    "            name, start = test_name_list[i]\n",
    "            if prv_classname == classnames[name_dict[name]]:\n",
    "                continue\n",
    "            \n",
    "            axarr[idx].imshow(x_test[i+j].reshape(1,time_steps*n_input))\n",
    "            #name, start = test_name_list[i]\n",
    "            #axarr[idx].set_title(classnames[name_dict[name]]+ \":\" + name )\n",
    "            axarr[idx].set_title(classnames[name_dict[name]].split('_')[0]+ \":\" + name )\n",
    "            axarr[idx].axis('off')\n",
    "            idx += 1\n",
    "            #print(idx)\n",
    "        prv_classname = classnames[name_dict[name]]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"T:/APIEncoding.pdf\", dpi=300)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
