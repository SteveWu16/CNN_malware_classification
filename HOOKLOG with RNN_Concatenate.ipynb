{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ATS_LAB\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets('C:/test/tensorflow/mnist/input_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unrolled through 28 time steps\n",
    "time_steps = 1\n",
    "#hidden LSTM units\n",
    "num_units = 128\n",
    "#rows of 28 pixels\n",
    "n_input = 128\n",
    "#learning rate for adam\n",
    "learning_rate = 0.001\n",
    "#mnist is meant to be classified in 10 classes(0-9).\n",
    "#n_classes=10\n",
    "#size of batch\n",
    "batch_size = 30\n",
    "\n",
    "test_rate = 0.1\n",
    "h_size = time_steps * n_input\n",
    "API_padding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%run ../MotifAnalysis/Hooklog3.ipynb\n",
    "%run Hooklog3.ipynb\n",
    "Hooklog = Hooklog3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# old data\n",
    "'''\n",
    "in_base_dir = \"C:/test/tensorflow/hooklog/\"\n",
    "in_directories = [\"allaple_woj_g_98_year2017\", # 16k\n",
    "                  \"somoto_woj_year2017\", # 17k\n",
    "                  \"morstar_g_54\", # 85k\n",
    "                  \"bettersurf_woj_g_137+\", # 106k\n",
    "                  \"elkern_woj_g_127\", # 44k\n",
    "                  \"lydra_woj_g_44\", # 30k\n",
    "                  \"mira_woj_g_107\", # 14k\n",
    "                  \"sytro_woj_g_166\" # 34k\n",
    "                 ]\n",
    "\n",
    "# new data\n",
    "in_base_dir = \"C:/Users/hsiao/Downloads/GitHub/Tensorflow/hooklogs/\"\n",
    "in_directories = [\n",
    "    \"allaple_woj_g_98_year2017\",\n",
    "    \"bettersurf_woj_g_137+\",\n",
    "    \"elkern_woj_g_127\",\n",
    "    \"fakeav_g_132\",\n",
    "    \"loring_g_15\",\n",
    "    \"morstar_g_54\",\n",
    "    \"rahack_g_39\",\n",
    "    \"sytro_woj_g_166\"\n",
    "]\n",
    "'''\n",
    "\n",
    "# yj\n",
    "#in_base_dir = \"C:/test/hooklog_family_yj/hooklog/\"\n",
    "in_base_dir = \"T:/ftp/hooklog/allhooklog/family/\"\n",
    "in_directories = None\n",
    "if in_directories is None:\n",
    "    import os\n",
    "    in_directories = next(os.walk(in_base_dir))[1]\n",
    "\n",
    "# real dirs\n",
    "in_directories = [in_base_dir + d + '/' for d in in_directories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 ['allaple_woj_g_98_year2017', 'bettersurf_woj_g_137+', 'elkern_woj_g_127', 'graftor_g_18', 'hotbar_g_32', 'kryptik_g_529', 'kryptik_g_547', 'loadmoney_g_183', 'loring_g_15', 'mydoom_g_13', 'rahack_g_39', 'sytro_woj_g_166', 'vobfus_g_111', 'zbot_g_37']\n"
     ]
    }
   ],
   "source": [
    "'''in_parseFirstPar = False\n",
    "\n",
    "# hooklog's freq\n",
    "#in_apifreq_dir = \"C:/Users/hsiao/Dropbox/notebook/MotifAnalysis/pickle/\"\n",
    "#in_apifreq_dir = \"C:/Users/hsiao/Downloads/GitHub/Tensorflow/pickles/\"\n",
    "#in_apifreq_dir = \"C:/test/hooklog_family_yj/pickle/\"\n",
    "\n",
    "#syscall's freq\n",
    "if ishooklog: # Windows API\n",
    "    #in_apifreq_dir = \"T:/ftp/Systemcall/hooklog_family/pickle/\"\n",
    "    in_apifreq_dir = \"T:/ftp/Tensorflow-master/pickles_goodfamily/\"\n",
    "elif isstrace: # syscall\n",
    "    in_apifreq_dir = \"T:/ftp/Systemcall/malware_syscall/pickle/\"\n",
    "else: # som 要改\n",
    "    in_apifreq_dir = \"T:/ftp/Systemcall/malware_syscall/pickle/\" \n",
    "\n",
    "#in_apifreq_dir = \"T:/ftp/Systemcall/hooklog_family/pickle/\"\n",
    "\n",
    "in_apifreq_dicts = [in_apifreq_dir + apifreq_dict_ + d.split(\"/\")[-2] + \".pickle\" for d in in_directories]\n",
    "\n",
    "# info\n",
    "n_classes = len(in_directories)\n",
    "classnames = list(map(lambda x: x.split(\"/\")[-2], in_directories))\n",
    "print(n_classes, classnames)'''\n",
    "\n",
    "in_parseFirstPar = False\n",
    "\n",
    "#in_apifreq_dir = \"C:/Users/hsiao/Dropbox/notebook/MotifAnalysis/pickle/\"\n",
    "#in_apifreq_dir = \"C:/Users/hsiao/Downloads/GitHub/Tensorflow/pickles/\"\n",
    "in_apifreq_dir = \"T:/ftp/hooklog/allhooklog/pickle/\"\n",
    "\n",
    "in_apifreq_dicts = [in_apifreq_dir+\"apifreq_dict_\"+d.split(\"/\")[-2]+\".pickle\" for d in in_directories]\n",
    "\n",
    "n_classes = len(in_directories)\n",
    "classnames = list(map(lambda x: x.split(\"/\")[-2], in_directories))\n",
    "print(n_classes, classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreateRemoteThread': ('CreateRemoteThread', 0, 0.0, 3, 3.896038118836955e-06), 'OpenThread': ('OpenThread', 1, 0.04, 14, 1.8181511221239123e-05), 'WinExec': ('WinExec', 2, 0.08, 15, 1.9480190594184772e-05), 'TerminateProcess': ('TerminateProcess', 3, 0.12, 21, 2.7272266831858684e-05), 'GetUrlCacheEntryInfo': ('GetUrlCacheEntryInfo', 4, 0.16, 33, 4.28564193072065e-05), 'WinHttpOpen': ('WinHttpOpen', 5, 0.2, 75, 9.740095297092386e-05), 'WinHttpConnect': ('WinHttpConnect', 6, 0.24, 75, 9.740095297092386e-05), 'WinHttpOpenRequest': ('WinHttpOpenRequest', 7, 0.28, 75, 9.740095297092386e-05), 'WinHttpSendRequest': ('WinHttpSendRequest', 8, 0.32, 170, 0.00022077549340076077), 'CreateProcess': ('CreateProcess', 9, 0.36, 426, 0.0005532374128748475), 'ExitProcess': ('ExitProcess', 10, 0.4, 618, 0.0008025838524804127), 'InternetOpen': ('InternetOpen', 11, 0.44, 779, 0.0010116712315246625), 'CreateProcessInternal': ('CreateProcessInternal', 12, 0.48, 791, 0.0010272553840000103), 'HttpSendRequest': ('HttpSendRequest', 13, 0.52, 856, 0.0011116695432414777), 'InternetConnect': ('InternetConnect', 14, 0.56, 912, 0.0011843955881264343), 'RegDeleteKey': ('RegDeleteKey', 15, 0.6, 1856, 0.0024103489161871293), 'OpenProcess': ('OpenProcess', 16, 0.64, 2019, 0.0026220336539772705), 'CreateThread': ('CreateThread', 17, 0.68, 3927, 0.005099913897557574), 'RegSetValue': ('RegSetValue', 18, 0.72, 16433, 0.021341198135615892), 'RegCreateKey': ('RegCreateKey', 19, 0.76, 18481, 0.024000893491408586), 'CopyFile': ('CopyFile', 20, 0.8, 27247, 0.035385116874650166), 'RegEnumValue': ('RegEnumValue', 21, 0.84, 51961, 0.067480678897629), 'LoadLibrary': ('LoadLibrary', 22, 0.88, 52652, 0.06837806634433445), 'CreateFile': ('CreateFile', 23, 0.92, 143427, 0.18626568642347596), 'DeleteFile': ('DeleteFile', 24, 0.96, 160671, 0.20866011353055078), 'RegQueryValue': ('RegQueryValue', 25, 1.0, 286476, 0.3720404720439785)}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "apifreq_dict = dict()\n",
    "_total = 0\n",
    "for pkf in in_apifreq_dicts:\n",
    "    with open(pkf, 'rb') as f:\n",
    "        this_dict = pickle.load(f)\n",
    "        for k in this_dict:\n",
    "            if k in apifreq_dict:\n",
    "                apifreq_dict[k] += this_dict[k]\n",
    "            else:\n",
    "                apifreq_dict[k] = this_dict[k]\n",
    "            _total += this_dict[k]\n",
    "\n",
    "s_dict = {item[0]: item for item in [(k, i, i/(len(apifreq_dict)-1), apifreq_dict[k], apifreq_dict[k]/_total) for i, k in enumerate(sorted(apifreq_dict, key = apifreq_dict.get, reverse=False))]}\n",
    "print(s_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T:/ftp/hooklog/allhooklog/family/allaple_woj_g_98_year2017/\n",
      "T:/ftp/hooklog/allhooklog/family/bettersurf_woj_g_137+/\n",
      "T:/ftp/hooklog/allhooklog/family/elkern_woj_g_127/\n",
      "T:/ftp/hooklog/allhooklog/family/graftor_g_18/\n",
      "T:/ftp/hooklog/allhooklog/family/hotbar_g_32/\n",
      "T:/ftp/hooklog/allhooklog/family/kryptik_g_529/\n",
      "T:/ftp/hooklog/allhooklog/family/kryptik_g_547/\n",
      "T:/ftp/hooklog/allhooklog/family/loadmoney_g_183/\n",
      "T:/ftp/hooklog/allhooklog/family/loring_g_15/\n",
      "T:/ftp/hooklog/allhooklog/family/mydoom_g_13/\n",
      "T:/ftp/hooklog/allhooklog/family/rahack_g_39/\n",
      "T:/ftp/hooklog/allhooklog/family/sytro_woj_g_166/\n",
      "T:/ftp/hooklog/allhooklog/family/vobfus_g_111/\n",
      "T:/ftp/hooklog/allhooklog/family/zbot_g_37/\n",
      "malware length minimum: 18\n",
      "name: T:/ftp/hooklog/allhooklog/family/vobfus_g_111/1d3cfb9bd2754df6db4994e07667c46a2b0c1f0eeee90511afe518c598c73dcf_3200.trace.hooklog\n",
      "---------------------------\n",
      "malware length maximum: 20766\n",
      "name: T:/ftp/hooklog/allhooklog/family/loadmoney_g_183/ffe3854f6b135356e619680964df8790_3220.trace.hooklog\n",
      "---------------------------\n",
      "count： 1940\n",
      "malware length average: 396.9139175257732\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "xx_train_list = list()\n",
    "yy_train_list = list()\n",
    "\n",
    "xx_test_list = list()\n",
    "yy_test_list = list()\n",
    "\n",
    "train_name_list = list()\n",
    "test_name_list = list()\n",
    "\n",
    "minimum = 0 #最小長度\n",
    "maximum = 0 #最大長度\n",
    "total = 0 #長度總和\n",
    "count = 0\n",
    "\n",
    "name_dict = dict()\n",
    "\n",
    "for label, in_dir in enumerate(in_directories):\n",
    "    print(in_dir)\n",
    "    hl_list = next(os.walk(in_dir))[2] # get all filenames in the in_directory\n",
    "    hl_list = [os.path.join(in_dir, f) for f in hl_list] # filepathname list\n",
    "    hl_list = list(filter(lambda f: f.endswith(\".hooklog\"), hl_list)) # in case some non-hooklog file in the folder\n",
    "    \n",
    "    #shuffle list\n",
    "    random.shuffle(hl_list) # <-------------- random here\n",
    "    test_size = int(len(hl_list)*test_rate)\n",
    "\n",
    "    for i, file in enumerate(hl_list):\n",
    "        hl3 = Hooklog(file, in_parseFirstPar)\n",
    "        li_li = list() # for hacking moving_window # 20171129\n",
    "        \n",
    "        #statistic\n",
    "        if count == 0:\n",
    "            minimum = len(hl3.li)\n",
    "            \n",
    "        \n",
    "        if len(hl3.li) < minimum:\n",
    "            minimum = len(hl3.li)\n",
    "            minimum_name = file\n",
    "        \n",
    "        if len(hl3.li) > maximum:\n",
    "            maximum = len(hl3.li)\n",
    "            maximum_name = file\n",
    "            \n",
    "        total += len(hl3.li)\n",
    "        count += 1\n",
    "        \n",
    "        \n",
    "        for start in range(0, len(hl3.li), h_size):\n",
    "            end = start + h_size\n",
    "            #print(hl3.digitname, start, end)\n",
    "            \n",
    "            li = list()\n",
    "            for (t, api) in hl3.li[start:end]:\n",
    "                li.append(s_dict[api][4]) # <-- encode\n",
    "            # hack\n",
    "            if len(li) < h_size:\n",
    "                #print(\"!!! change img_cols to a smaller number\", len(hl3.li))\n",
    "                #print(hl3.digitname, \"has smaller size\", len(hl3.li), \"need img_cols\", str(img_cols))\n",
    "                if API_padding:\n",
    "                    #print(\"padding 1.0\", str(img_cols - len(hl3.li)))\n",
    "                    for _ in range(h_size - len(li)):\n",
    "                        li.append(1.0)\n",
    "            li_li.append(li)\n",
    "\n",
    "            if(i < test_size):\n",
    "                xx_test_list.extend([li])\n",
    "                a = [0] * n_classes\n",
    "                a[label] = 1\n",
    "                yy_test_list.extend([a])\n",
    "                test_name_list.append((hl3.digitname, start))\n",
    "                name_dict[hl3.digitname] = label\n",
    "            else:\n",
    "                xx_train_list.extend([li])\n",
    "                a = [0] * n_classes\n",
    "                a[label] = 1\n",
    "                yy_train_list.extend([a])\n",
    "                train_name_list.append((hl3.digitname, start))\n",
    "                name_dict[hl3.digitname] = label\n",
    "                \n",
    "            break; # stop moving window!! Must break!\n",
    "print (\"malware length minimum:\", minimum)\n",
    "print(\"name:\",minimum_name)\n",
    "print(\"---------------------------\")\n",
    "print (\"malware length maximum:\", maximum)\n",
    "print(\"name:\",maximum_name)\n",
    "print(\"---------------------------\")\n",
    "print(\"count：\",count)\n",
    "print (\"malware length average:\", float(total)/float(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06837806634433445, 0.06837806634433445, 0.06837806634433445, 0.06837806634433445, 0.06837806634433445, 0.06837806634433445, 0.06837806634433445, 0.06837806634433445, 0.3720404720439785, 0.3720404720439785, 0.06837806634433445, 0.3720404720439785, 0.06837806634433445, 0.18626568642347596, 0.3720404720439785, 0.3720404720439785, 0.067480678897629, 0.06837806634433445, 0.3720404720439785, 0.06837806634433445, 0.06837806634433445, 0.06837806634433445, 0.06837806634433445, 0.06837806634433445, 0.18626568642347596, 0.18626568642347596, 0.024000893491408586, 0.021341198135615892, 0.024000893491408586, 0.021341198135615892, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.005099913897557574, 0.005099913897557574, 0.3720404720439785, 0.005099913897557574, 0.005099913897557574, 0.005099913897557574, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.3720404720439785, 0.18626568642347596, 0.18626568642347596, 0.18626568642347596, 0.18626568642347596, 0.18626568642347596, 0.18626568642347596, 0.18626568642347596, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(1751, 128)\n",
      "(1751, 14)\n",
      "(128,)\n",
      "[0.06837807 0.06837807 0.06837807 0.06837807 0.06837807 0.06837807\n",
      " 0.06837807 0.06837807 0.37204047 0.37204047 0.06837807 0.37204047\n",
      " 0.06837807 0.18626569 0.37204047 0.37204047 0.06748068 0.06837807\n",
      " 0.37204047 0.06837807 0.06837807 0.06837807 0.06837807 0.06837807\n",
      " 0.18626569 0.18626569 0.02400089 0.0213412  0.02400089 0.0213412\n",
      " 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047\n",
      " 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047\n",
      " 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047\n",
      " 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047\n",
      " 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047\n",
      " 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047\n",
      " 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047\n",
      " 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047\n",
      " 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047\n",
      " 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047 0.37204047\n",
      " 0.37204047 0.37204047 0.00509991 0.00509991 0.37204047 0.00509991\n",
      " 0.00509991 0.00509991 0.37204047 0.37204047 0.37204047 0.37204047\n",
      " 0.18626569 0.18626569 0.18626569 0.18626569 0.18626569 0.18626569\n",
      " 0.18626569 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#x_train = np.ndarray(shape = (len(xx_train_list), img_rows, img_cols), buffer = np.array(xx_train_list))\n",
    "#y_train = np.array(yy_train_list)\n",
    "print(xx_train_list[0])\n",
    "x_train = np.ndarray(shape = (len(xx_train_list), time_steps * n_input), buffer = np.array(xx_train_list))\n",
    "y_train = np.array(yy_train_list)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_train[0].shape)\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 128)\n",
      "(189, 14)\n"
     ]
    }
   ],
   "source": [
    "#x_test = x_train.copy()\n",
    "#y_test = y_train.copy()\n",
    "\n",
    "x_test= np.ndarray(shape = (len(xx_test_list), time_steps * n_input), buffer = np.array(xx_test_list))\n",
    "y_test = np.array(yy_test_list)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#weights and biases of appropriate shape to accomplish above task\n",
    "out_weights=tf.Variable(tf.random_normal([num_units*time_steps,n_classes]))\n",
    "out_bias=tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(128, 14) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=(14,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(out_weights)\n",
    "print(out_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input image placeholder\n",
    "x=tf.placeholder(\"float\",[None,time_steps,n_input])\n",
    "#input label placeholder\n",
    "y=tf.placeholder(\"float\",[None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 128)\n",
      "(?, 14)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input_x=tf.unstack(x, time_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'unstack:0' shape=(?, 128) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_rnn.shape： (?, 128)\n",
      "output_rnn.shape： (?, 128)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"add:0\", shape=(?, 14), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-18-84b09854caab>:39: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#defining the network\n",
    "with tf.variable_scope('anystring', reuse = tf.AUTO_REUSE):\n",
    "    lstm_layer=tf.contrib.rnn.BasicLSTMCell(num_units,forget_bias=1)\n",
    "    outputs,_=tf.nn.static_rnn(lstm_layer,input_x,dtype=\"float32\") # Mike: I change it to tf.nn.\n",
    "    \n",
    "    #converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "    prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "    \n",
    "    \n",
    "    #loss_function\n",
    "    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "    #optimization\n",
    "    opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    #model evaluation\n",
    "    correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "'''\n",
    "lstm_layer=tf.contrib.rnn.BasicLSTMCell(num_units,forget_bias=0.5)\n",
    "outputs,_=tf.nn.static_rnn(lstm_layer,input_x,dtype=\"float32\") # Mike: I change it to tf.nn.\n",
    "\n",
    "#print(\"outputs\")\n",
    "#print(outputs)\n",
    "\n",
    "    #concat output\n",
    "output_rnn = tf.concat(outputs, axis=1)\n",
    "print(\"output_rnn.shape：\",output_rnn.shape)\n",
    "output_rnn = tf.reshape(output_rnn, [-1 , num_units*time_steps])\n",
    "print(\"output_rnn.shape：\",output_rnn.shape)\n",
    "\n",
    "    #converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "#prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "#prediction=tf.matmul(output_rnn[-1],out_weights)+out_bias\n",
    "prediction=tf.matmul(output_rnn,out_weights)+out_bias\n",
    "print(type(prediction),prediction)\n",
    "    \n",
    "    #loss_function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "    #optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    #model evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(type(output_rnn[-1]))\\nprint(output_rnn.shape)\\nprint(output_rnn)\\nprint(output_rnn[-1].shape)\\nprint(output_rnn[-1])\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(type(output_rnn[-1]))\n",
    "print(output_rnn.shape)\n",
    "print(output_rnn)\n",
    "print(output_rnn[-1].shape)\n",
    "print(output_rnn[-1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_next_batch(x_test, y_test, seq, start, batch_size):\n",
    "    end = start + batch_size\n",
    "    if end > len(x_test):\n",
    "        # Shuffle the data\n",
    "        perm = np.arange(len(x_test))\n",
    "        np.random.shuffle(perm)\n",
    "        # Start next epoch\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        seq = perm\n",
    "    return x_test[seq][start:end], y_test[seq][start:end], seq, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(x_train.shape)\\nprint(len(x_train))\\nprint(y_train.shape)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(x_train.shape)\n",
    "print(len(x_train))\n",
    "print(y_train.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch: 10 Accuracy: 1.0 Loss: 0.00034374683\n",
      "For epoch: 20 Accuracy: 0.0 Loss: 8.499226\n",
      "For epoch: 30 Accuracy: 0.0 Loss: 1.3029693\n",
      "For epoch: 40 Accuracy: 0.0 Loss: 3.82894\n",
      "For epoch: 50 Accuracy: 0.0 Loss: 3.4296248\n",
      "For epoch: 60 Accuracy: 0.16666667 Loss: 3.9186146\n",
      "For epoch: 70 Accuracy: 0.2 Loss: 2.1214807\n",
      "For epoch: 80 Accuracy: 0.76666665 Loss: 1.328966\n",
      "For epoch: 90 Accuracy: 0.4 Loss: 1.9281437\n",
      "For epoch: 100 Accuracy: 0.6 Loss: 1.4484923\n",
      "For epoch: 110 Accuracy: 0.6 Loss: 1.5010272\n",
      "For epoch: 120 Accuracy: 0.73333335 Loss: 1.0826266\n",
      "For epoch: 130 Accuracy: 0.56666666 Loss: 1.2968308\n",
      "For epoch: 140 Accuracy: 0.76666665 Loss: 0.9664729\n",
      "For epoch: 150 Accuracy: 0.9 Loss: 0.5970244\n",
      "For epoch: 160 Accuracy: 0.6666667 Loss: 1.1056327\n",
      "For epoch: 170 Accuracy: 0.6666667 Loss: 1.0768338\n",
      "For epoch: 180 Accuracy: 0.76666665 Loss: 0.83838016\n",
      "For epoch: 190 Accuracy: 0.76666665 Loss: 0.83836377\n",
      "For epoch: 200 Accuracy: 0.8 Loss: 0.7024589\n",
      "For epoch: 210 Accuracy: 0.7 Loss: 0.7570035\n",
      "For epoch: 220 Accuracy: 0.8 Loss: 0.60840267\n",
      "For epoch: 230 Accuracy: 0.8 Loss: 0.70529807\n",
      "For epoch: 240 Accuracy: 0.8 Loss: 0.73322886\n",
      "For epoch: 250 Accuracy: 0.8333333 Loss: 0.41235918\n",
      "For epoch: 260 Accuracy: 0.93333334 Loss: 0.4156259\n",
      "For epoch: 270 Accuracy: 0.8 Loss: 0.6019433\n",
      "For epoch: 280 Accuracy: 0.73333335 Loss: 0.69574547\n",
      "For epoch: 290 Accuracy: 0.8666667 Loss: 0.49030885\n",
      "For epoch: 300 Accuracy: 0.76666665 Loss: 0.51469964\n",
      "For epoch: 310 Accuracy: 0.8 Loss: 0.69832385\n",
      "For epoch: 320 Accuracy: 0.76666665 Loss: 0.8796779\n",
      "For epoch: 330 Accuracy: 0.9 Loss: 0.42058563\n",
      "For epoch: 340 Accuracy: 0.9 Loss: 0.40207213\n",
      "For epoch: 350 Accuracy: 0.9 Loss: 0.43316576\n",
      "For epoch: 360 Accuracy: 0.9 Loss: 0.46325842\n",
      "For epoch: 370 Accuracy: 0.8 Loss: 0.6201717\n",
      "For epoch: 380 Accuracy: 0.8 Loss: 0.4183411\n",
      "For epoch: 390 Accuracy: 0.8666667 Loss: 0.5171614\n",
      "For epoch: 400 Accuracy: 0.96666664 Loss: 0.1791665\n",
      "For epoch: 410 Accuracy: 0.93333334 Loss: 0.30929196\n",
      "For epoch: 420 Accuracy: 0.8333333 Loss: 0.5096881\n",
      "For epoch: 430 Accuracy: 0.9 Loss: 0.26160166\n",
      "For epoch: 440 Accuracy: 0.8 Loss: 0.5141784\n",
      "For epoch: 450 Accuracy: 0.8666667 Loss: 0.5317342\n",
      "For epoch: 460 Accuracy: 0.93333334 Loss: 0.26047295\n",
      "For epoch: 470 Accuracy: 0.9 Loss: 0.42936897\n",
      "For epoch: 480 Accuracy: 0.8666667 Loss: 0.339933\n",
      "For epoch: 490 Accuracy: 0.93333334 Loss: 0.27729088\n",
      "total time: 0.8677952289581299\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "epoch = 1\n",
    "\n",
    "b = 0\n",
    "seq = np.arange(len(x_train))\n",
    "t0 = time.time()\n",
    "\n",
    "while epoch < 500:\n",
    "    #batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)\n",
    "    batch_x, batch_y, seq, b = my_next_batch(x_train, y_train, seq, b, batch_size)\n",
    "    batch_x = batch_x.reshape((batch_size, time_steps, n_input))\n",
    "    #print(type(batch_x), batch_x.shape, type(batch_y), batch_y.shape)\n",
    "\n",
    "    sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "    if epoch %10 == 0:\n",
    "        acc = sess.run(accuracy, feed_dict = {x: batch_x, y: batch_y})\n",
    "        los = sess.run(loss, feed_dict = {x: batch_x, y: batch_y})\n",
    "        print(\"For epoch:\", epoch, \"Accuracy:\", acc, \"Loss:\",los)\n",
    "        #if acc >= 0.95: break # hack\n",
    "\n",
    "    epoch += 1\n",
    "    \n",
    "t1 = time.time()\n",
    "\n",
    "print(\"total time:\", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8994709\n"
     ]
    }
   ],
   "source": [
    "#calculating test accuracy\n",
    "#test_data = mnist.test.images[:128].reshape((-1, time_steps, n_input))\n",
    "#test_label = mnist.test.labels[:128]\n",
    "test_data = x_test.reshape((-1, time_steps, n_input))\n",
    "test_label = y_test\n",
    "print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict = {x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 train_able_variables in the Graph\n",
      "<tf.Variable 'Variable:0' shape=(128, 14) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=(14,) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "vs = tf.trainable_variables()\n",
    "print('There are', len(vs), 'train_able_variables in the Graph')\n",
    "for v in vs:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some other data ##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 128 into shape (8,8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-391aa6bdb83d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwhich\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#first_array = x_test[which].reshape(8, int(num_units/8))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfirst_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwhich\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#print(test_name_list[which])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#print(x_test[which])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 128 into shape (8,8)"
     ]
    }
   ],
   "source": [
    "\n",
    "# the image\n",
    "which = 0\n",
    "#first_array = x_test[which].reshape(8, int(num_units/8))\n",
    "first_array = x_test[which].reshape(8, 8)\n",
    "#print(test_name_list[which])\n",
    "#print(x_test[which])\n",
    "plt.imshow(first_array) # color viridis\n",
    "plt.savefig(\"T:/visualize1.pdf\", dpi=300)\n",
    "plt.matshow(first_array, cmap = plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.savefig(\"T:/visualize2.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(n_classes)\n",
    "\n",
    "s_x, s_y = len(x_test), 1\n",
    "f, axarr = plt.subplots(n_classes, s_y, figsize=(10,n_classes))\n",
    "prv_classname = None\n",
    "idx = 0\n",
    "for i in range(s_x):\n",
    "    for j in range(s_y):\n",
    "            \n",
    "        if s_y != 1:\n",
    "            axarr[i,j].imshow(x_test[i+j].reshape(1,time_steps*n_input))\n",
    "            name, start = test_name_list[i+j]\n",
    "            axarr[i,j].set_title(classnames[name_dict[name]]+ \":\" + name )\n",
    "        else:\n",
    "            name, start = test_name_list[i]\n",
    "            if prv_classname == classnames[name_dict[name]]:\n",
    "                continue\n",
    "            \n",
    "            axarr[idx].imshow(x_test[i+j].reshape(1,time_steps*n_input))\n",
    "            #name, start = test_name_list[i]\n",
    "            #axarr[idx].set_title(classnames[name_dict[name]]+ \":\" + name )\n",
    "            axarr[idx].set_title(classnames[name_dict[name]].split('_')[0]+ \":\" + name )\n",
    "            axarr[idx].axis('off')\n",
    "            idx += 1\n",
    "            #print(idx)\n",
    "        prv_classname = classnames[name_dict[name]]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"T:/APIEncoding.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
