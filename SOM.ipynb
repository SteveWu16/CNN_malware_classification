{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PAD': ('Padding', 0, 0.0, 0, 0.0)}\n",
      "{'Padding': ('Padding', 0, 0.0, 0, 0.0), 'CreateRemoteThread': ('CreateRemoteThread', 1, 0.038461538461538464, 3, 1.0472081430905207e-05), 'OpenThread': ('OpenThread', 2, 0.07692307692307693, 14, 4.8869713344224296e-05), 'WinExec': ('WinExec', 3, 0.11538461538461539, 15, 5.236040715452604e-05), 'TerminateProcess': ('TerminateProcess', 4, 0.15384615384615385, 21, 7.330457001633645e-05), 'GetUrlCacheEntryInfo': ('GetUrlCacheEntryInfo', 5, 0.19230769230769232, 33, 0.00011519289573995727), 'WinHttpOpen': ('WinHttpOpen', 6, 0.23076923076923078, 75, 0.0002618020357726302), 'WinHttpConnect': ('WinHttpConnect', 7, 0.2692307692307692, 75, 0.0002618020357726302), 'WinHttpOpenRequest': ('WinHttpOpenRequest', 8, 0.3076923076923077, 75, 0.0002618020357726302), 'WinHttpSendRequest': ('WinHttpSendRequest', 9, 0.34615384615384615, 170, 0.000593417947751295), 'CreateProcess': ('CreateProcess', 10, 0.38461538461538464, 426, 0.0014870355631885393), 'ExitProcess': ('ExitProcess', 11, 0.4230769230769231, 618, 0.0021572487747664724), 'InternetOpen': ('InternetOpen', 12, 0.46153846153846156, 779, 0.002719250478225052), 'CreateProcessInternal': ('CreateProcessInternal', 13, 0.5, 791, 0.002761138803948673), 'HttpSendRequest': ('HttpSendRequest', 14, 0.5384615384615384, 856, 0.002988033901618286), 'InternetConnect': ('InternetConnect', 15, 0.5769230769230769, 912, 0.003183512754995183), 'RegDeleteKey': ('RegDeleteKey', 16, 0.6153846153846154, 1856, 0.006478727711920021), 'OpenProcess': ('OpenProcess', 17, 0.6538461538461539, 2019, 0.007047710802999204), 'CreateThread': ('CreateThread', 18, 0.6923076923076923, 3927, 0.013707954593054916), 'RegSetValue': ('RegSetValue', 19, 0.7307692307692307, 16433, 0.05736257138468842), 'RegCreateKey': ('RegCreateKey', 20, 0.7692307692307693, 18481, 0.06451151230818637), 'CopyFile': ('CopyFile', 21, 0.8076923076923077, 27247, 0.09511093424929139), 'RegEnumValue': ('RegEnumValue', 22, 0.8461538461538461, 51961, 0.1813799410770885), 'LoadLibrary': ('LoadLibrary', 23, 0.8846153846153846, 52652, 0.18379201050000699), 'CreateFile': ('CreateFile', 24, 0.9230769230769231, 143427, 0.500659741130147), 'DeleteFile': ('DeleteFile', 25, 0.9615384615384616, 160671, 0.5608532651949901), 'RegQueryValue': ('RegQueryValue', 26, 1.0, 286476, 1.0)}\n",
      "C:/Users/admin/Desktop/DATASET/family/allaple_woj_g_98_year2017/\n",
      "C:/Users/admin/Desktop/DATASET/family/bettersurf_woj_g_137+/\n",
      "C:/Users/admin/Desktop/DATASET/family/elkern_woj_g_127/\n",
      "C:/Users/admin/Desktop/DATASET/family/graftor_g_18/\n",
      "C:/Users/admin/Desktop/DATASET/family/hotbar_g_32/\n",
      "C:/Users/admin/Desktop/DATASET/family/kryptik_g_529/\n",
      "C:/Users/admin/Desktop/DATASET/family/kryptik_g_547/\n",
      "C:/Users/admin/Desktop/DATASET/family/loadmoney_g_183/\n",
      "C:/Users/admin/Desktop/DATASET/family/loring_g_15/\n",
      "C:/Users/admin/Desktop/DATASET/family/mydoom_g_13/\n",
      "C:/Users/admin/Desktop/DATASET/family/rahack_g_39/\n",
      "C:/Users/admin/Desktop/DATASET/family/sytro_woj_g_166/\n",
      "C:/Users/admin/Desktop/DATASET/family/vobfus_g_111/\n",
      "C:/Users/admin/Desktop/DATASET/family/zbot_g_37/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# %run LoadModel.ipynb\n",
    "\n",
    "# import math\n",
    "# test_rate = 0.1\n",
    "# img_rows, img_cols = 1, 128\n",
    "# API_padding = False\n",
    "# API_padding = True # with API padding and without API moving window\n",
    "# API_moving = API_padding = True # with API padding and API moving window\n",
    "\n",
    "# %run C:/Users/admin/Dropbox/Code/example/Hooklog3.ipynb\n",
    "# Hooklog = Hooklog3\n",
    "\n",
    "# in_directories = [\"C:/Users/admin/Desktop/DATASET/family/allaple_woj_g_98_year2017/\", \n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/bettersurf_woj_g_137+/\", \n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/elkern_woj_g_127/\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/graftor_g_18/\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/hotbar_g_32/\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/kryptik_g_529/\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/kryptik_g_547/\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/loadmoney_g_183/\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/loring_g_15/\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/mydoom_g_13/\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/rahack_g_39/\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/sytro_woj_g_166/\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/vobfus_g_111/\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/zbot_g_37/\",\n",
    "#                  ]\n",
    "# in_parseFirstPar = False\n",
    "\n",
    "# in_apifreq_dicts = [\"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_allaple_woj_g_98_year2017.pickle\", \n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_bettersurf_woj_g_137+.pickle\", \n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_elkern_woj_g_127.pickle\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_graftor_g_18.pickle\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_hotbar_g_32.pickle\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_kryptik_g_529.pickle\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_kryptik_g_547.pickle\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_loadmoney_g_183.pickle\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_loring_g_15.pickle\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_mydoom_g_13.pickle\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_rahack_g_39.pickle\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_sytro_woj_g_166.pickle\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_vobfus_g_111.pickle\",\n",
    "#                   \"C:/Users/admin/Desktop/DATASET/family/pickle/apifreq_dict_zbot_g_37.pickle\"\n",
    "#                    ]\n",
    "\n",
    "# num_classes = len(in_directories)\n",
    "# classnames = list(map(lambda x: x.split(\"/\")[-2], in_directories))\n",
    "# # print(num_classes, classnames)\n",
    "\n",
    "# adict = dict()\n",
    "\n",
    "# adict['PAD'] = ('Padding', 0, 0.0, 0, 0.0)\n",
    "# print(adict)\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# apifreq_dict = dict()\n",
    "# _total = 0\n",
    "\n",
    "# for pkf in in_apifreq_dicts:\n",
    "#     with open(pkf, 'rb') as f:\n",
    "#         this_dict = pickle.load(f)\n",
    "#         this_dict['Padding'] = 0\n",
    "#         for k in this_dict:\n",
    "#             if k in apifreq_dict:\n",
    "#                 apifreq_dict[k] += this_dict[k]\n",
    "#             else:\n",
    "#                 apifreq_dict[k] = this_dict[k]\n",
    "#             _total += this_dict[k]\n",
    "\n",
    "# s_dict = {item[0]: item for item in [(k, i, i/(len(apifreq_dict)-1), apifreq_dict[k], apifreq_dict[k]/286476) for i, k in enumerate(sorted(apifreq_dict, key = apifreq_dict.get, reverse=False))]}\n",
    "# print(s_dict)\n",
    "\n",
    "# import os\n",
    "# import random\n",
    "\n",
    "# xx_train_list = list()\n",
    "# yy_train_list = list()\n",
    "\n",
    "# x_train_raw = list()\n",
    "\n",
    "# xx_test_list = list()\n",
    "# yy_test_list = list()\n",
    "\n",
    "# x_test_raw = list()\n",
    "\n",
    "# train_name_list = list()\n",
    "# test_name_list = list()\n",
    "\n",
    "# minimum = 0 #最小長度\n",
    "# maximum = 0 #最大長度\n",
    "# total = 0 #長度總和\n",
    "# count = 0\n",
    "\n",
    "# name_dict = dict()\n",
    "# W_Size = 128\n",
    "# h_size = W_Size\n",
    "# n_classes = num_classes\n",
    "\n",
    "# for label, in_dir in enumerate(in_directories):\n",
    "#     print(in_dir)\n",
    "#     hl_list = next(os.walk(in_dir))[2] # get all filenames in the in_directory\n",
    "#     hl_list = [os.path.join(in_dir, f) for f in hl_list] # filepathname list\n",
    "#     hl_list = list(filter(lambda f: f.endswith(\".hooklog\"), hl_list)) # in case some non-hooklog file in the folder\n",
    "    \n",
    "#     #shuffle list\n",
    "#     random.shuffle(hl_list) # <-------------- random here\n",
    "#     test_size = int(len(hl_list)*test_rate)\n",
    "\n",
    "#     for i, file in enumerate(hl_list):\n",
    "#         hl3 = Hooklog(file, in_parseFirstPar)\n",
    "        \n",
    "#         li_li = list() # for hacking moving_window # 20171129\n",
    "        \n",
    "#         #statistic\n",
    "#         if count == 0:\n",
    "#             minimum = len(hl3.li)\n",
    "        \n",
    "#         if len(hl3.li) < minimum:\n",
    "#             minimum = len(hl3.li)\n",
    "#             minimum_name = file\n",
    "        \n",
    "#         if len(hl3.li) > maximum:\n",
    "#             maximum = len(hl3.li)\n",
    "#             maximum_name = file\n",
    "            \n",
    "#         total += len(hl3.li)\n",
    "#         count += 1\n",
    "        \n",
    "#         for start in range(0, len(hl3.li), h_size):\n",
    "#             end = start + h_size\n",
    "#             #print(hl3.digitname, start, end)\n",
    "            \n",
    "#             li = list()\n",
    "#             raw_li = list()\n",
    "#             for (t, api) in hl3.li[start:end]:\n",
    "#                 li.append(s_dict[api][0]) # <-- encode\n",
    "#                 raw_li.append(s_dict[api][0])\n",
    "#             # hack\n",
    "#             if len(li) < h_size:\n",
    "#                 #print(\"!!! change img_cols to a smaller number\", len(hl3.li))\n",
    "#                 #print(hl3.digitname, \"has smaller size\", len(hl3.li), \"need img_cols\", str(img_cols))\n",
    "#                 if API_padding:\n",
    "#                     #print(\"padding 1.0\", str(img_cols - len(hl3.li)))\n",
    "#                     for _ in range(h_size - len(li)):\n",
    "#                         li.append(0)\n",
    "#                         raw_li.append('Padding')\n",
    "#             li_li.append(li)\n",
    "            \n",
    "#             if(i < test_size):\n",
    "#                 xx_test_list.extend([li])\n",
    "#                 x_test_raw.extend([raw_li])\n",
    "#                 a = [0] * n_classes\n",
    "#                 a[label] = 1\n",
    "#                 yy_test_list.extend([a])\n",
    "#                 test_name_list.append((hl3.digitname, start))\n",
    "#                 name_dict[hl3.digitname] = label\n",
    "#             else:\n",
    "#                 xx_train_list.extend([li])\n",
    "#                 x_train_raw.extend([raw_li])\n",
    "#                 a = [0] * n_classes\n",
    "#                 a[label] = 1\n",
    "#                 yy_train_list.extend([a])\n",
    "#                 train_name_list.append((hl3.digitname, start))\n",
    "#                 name_dict[hl3.digitname] = label\n",
    "                \n",
    "#             break; # stop moving window!! Must break!\n",
    "\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "data_list = list()\n",
    "# 開啟 CSV 檔案\n",
    "with open('C:/Users/admin/Desktop/DATASET/family/4gram.csv', newline='') as csvfile:\n",
    "\n",
    "  # 讀取 CSV 檔案內容\n",
    "  rows = csv.reader(csvfile)\n",
    "\n",
    "  # 以迴圈輸出每一列\n",
    "  for row in rows:\n",
    "    data_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1940\n"
     ]
    }
   ],
   "source": [
    "print(len(data_list[1:]))\n",
    "n = list()\n",
    "d = list()\n",
    "\n",
    "for i in range(len(data_list[1:])):\n",
    "    n.append(data_list[i+1][0])\n",
    "    d.append(list(map(int, data_list[i+1][1:])))\n",
    "\n",
    "column_name = list()\n",
    "column_name.append(data_list[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0316c4-3320\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(n[0])\n",
    "print(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gram4 = list()\n",
    "# temp4 = list()\n",
    "\n",
    "# for i in range(len(x_train_raw)):\n",
    "#     for j in range(len(x_train_raw[i])-3):\n",
    "#         for k in range(4):\n",
    "#             temp4.append(x_train_raw[i][j+k])\n",
    "#         if temp4 not in gram4:\n",
    "#             gram4.append(temp4)\n",
    "#             temp4 = list()\n",
    "#         else:\n",
    "#             temp4 = list()\n",
    "            \n",
    "# for i in range(len(x_test_raw)):\n",
    "#     for j in range(len(x_test_raw[i])-3):\n",
    "#         for k in range(4):\n",
    "#             temp4.append(x_test_raw[i][j+k])\n",
    "#         if temp4 not in gram4:\n",
    "#             gram4.append(temp4)\n",
    "#             temp4 = list()\n",
    "#         else:\n",
    "#             temp4 = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot_4gram = list()\n",
    "# temp4 = list()\n",
    "\n",
    "# for i in range(len(x_train_raw)):\n",
    "#     a = [0] * len(gram4)\n",
    "#     for j in range(len(x_train_raw[i])-3):\n",
    "#         for k in range(4):\n",
    "#             temp4.append(x_train_raw[i][j+k])\n",
    "#         if temp4 not in gram4:\n",
    "#             temp4 = list()\n",
    "#         else:\n",
    "#             a[gram4.index(temp4)] = 1\n",
    "#             temp4 = list()\n",
    "#     onehot_4gram.append(a)\n",
    "    \n",
    "# for i in range(len(x_test_raw)):\n",
    "#     a = [0] * len(gram4)\n",
    "#     for j in range(len(x_test_raw[i])-3):\n",
    "#         for k in range(4):\n",
    "#             temp4.append(x_test_raw[i][j+k])\n",
    "#         if temp4 not in gram4:\n",
    "#             temp4 = list()\n",
    "#         else:\n",
    "#             a[gram4.index(temp4)] = 1\n",
    "#             temp4 = list()\n",
    "#     onehot_4gram.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_name_li = list()\n",
    "\n",
    "# for i in range(len(train_name_list)):\n",
    "#     all_name_li.extend([train_name_list[i][0]])\n",
    "# for i in range(len(test_name_list)):\n",
    "#     all_name_li.extend([test_name_list[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    "class SOM(object):\n",
    "    \"\"\"\n",
    "    2-D Self-Organizing Map with Gaussian Neighbourhood function\n",
    "    and linearly decreasing learning rate.\n",
    "    \"\"\"\n",
    " \n",
    "    #To check if the SOM has been trained\n",
    "    _trained = False\n",
    " \n",
    "    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):\n",
    "        \"\"\"\n",
    "        Initializes all necessary components of the TensorFlow\n",
    "        Graph.\n",
    " \n",
    "        m X n are the dimensions of the SOM. 'n_iterations' should\n",
    "        should be an integer denoting the number of iterations undergone\n",
    "        while training.\n",
    "        'dim' is the dimensionality of the training inputs.\n",
    "        'alpha' is a number denoting the initial time(iteration no)-based\n",
    "        learning rate. Default value is 0.3\n",
    "        'sigma' is the the initial neighbourhood value, denoting\n",
    "        the radius of influence of the BMU while training. By default, its\n",
    "        taken to be half of max(m, n).\n",
    "        \"\"\"\n",
    " \n",
    "        #Assign required variables first\n",
    "        self._m = m\n",
    "        self._n = n\n",
    "        if alpha is None:\n",
    "            alpha = 0.3\n",
    "        else:\n",
    "            alpha = float(alpha)\n",
    "        if sigma is None:\n",
    "            sigma = max(m, n) / 2.0\n",
    "        else:\n",
    "            sigma = float(sigma)\n",
    "        self._n_iterations = abs(int(n_iterations))\n",
    " \n",
    "        ##INITIALIZE GRAPH\n",
    "        self._graph = tf.Graph()\n",
    " \n",
    "        ##POPULATE GRAPH WITH NECESSARY COMPONENTS\n",
    "        with self._graph.as_default():\n",
    " \n",
    "            ##VARIABLES AND CONSTANT OPS FOR DATA STORAGE\n",
    " \n",
    "            #Randomly initialized weightage vectors for all neurons,\n",
    "            #stored together as a matrix Variable of size [m*n, dim]\n",
    "            self._weightage_vects = tf.Variable(tf.random_normal(\n",
    "                [m*n, dim]))\n",
    " \n",
    "            #Matrix of size [m*n, 2] for SOM grid locations\n",
    "            #of neurons\n",
    "            self._location_vects = tf.constant(np.array(\n",
    "                list(self._neuron_locations(m, n))))\n",
    " \n",
    "            ##PLACEHOLDERS FOR TRAINING INPUTS\n",
    "            #We need to assign them as attributes to self, since they\n",
    "            #will be fed in during training\n",
    " \n",
    "            #The training vector\n",
    "            self._vect_input = tf.placeholder(\"float\", [dim])\n",
    "            #Iteration number\n",
    "            self._iter_input = tf.placeholder(\"float\")\n",
    " \n",
    "            ##CONSTRUCT TRAINING OP PIECE BY PIECE\n",
    "            #Only the final, 'root' training op needs to be assigned as\n",
    "            #an attribute to self, since all the rest will be executed\n",
    "            #automatically during training\n",
    " \n",
    "            #To compute the Best Matching Unit given a vector\n",
    "            #Basically calculates the Euclidean distance between every\n",
    "            #neuron's weightage vector and the input, and returns the\n",
    "            #index of the neuron which gives the least value\n",
    "            bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
    "                tf.pow(tf.subtract(self._weightage_vects, tf.stack(\n",
    "                    [self._vect_input for i in range(m*n)])), 2), 1)),\n",
    "                                  0)\n",
    " \n",
    "            #This will extract the location of the BMU based on the BMU's\n",
    "            #index\n",
    "            slice_input = tf.pad(tf.reshape(bmu_index, [1]),\n",
    "                                 np.array([[0, 1]]))\n",
    "            bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n",
    "                                          tf.constant(np.array([1, 2]), dtype=tf.int64)),\n",
    "                                 [2])\n",
    " \n",
    "            #To compute the alpha and sigma values based on iteration\n",
    "            #number\n",
    "            learning_rate_op = tf.subtract(1.0, tf.div(self._iter_input,\n",
    "                                                  self._n_iterations))\n",
    "            _alpha_op = tf.multiply(alpha, learning_rate_op)\n",
    "            _sigma_op = tf.multiply(sigma, learning_rate_op)\n",
    " \n",
    "            #Construct the op that will generate a vector with learning\n",
    "            #rates for all neurons, based on iteration number and location\n",
    "            #wrt BMU.\n",
    "            bmu_distance_squares = tf.reduce_sum(tf.pow(tf.subtract(\n",
    "                self._location_vects, tf.stack(\n",
    "                    [bmu_loc for i in range(m*n)])), 2), 1)\n",
    "            neighbourhood_func = tf.exp(tf.negative(tf.div(tf.cast(\n",
    "                bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
    "            learning_rate_op = tf.multiply(_alpha_op, neighbourhood_func)\n",
    " \n",
    "            #Finally, the op that will use learning_rate_op to update\n",
    "            #the weightage vectors of all neurons based on a particular\n",
    "            #input\n",
    "            learning_rate_multiplier = tf.stack([tf.tile(tf.slice(\n",
    "                learning_rate_op, np.array([i]), np.array([1])), [dim])\n",
    "                                               for i in range(m*n)])\n",
    "            weightage_delta = tf.multiply(\n",
    "                learning_rate_multiplier,\n",
    "                tf.subtract(tf.stack([self._vect_input for i in range(m*n)]),\n",
    "                       self._weightage_vects))                                         \n",
    "            new_weightages_op = tf.add(self._weightage_vects,\n",
    "                                       weightage_delta)\n",
    "            self._training_op = tf.assign(self._weightage_vects,\n",
    "                                          new_weightages_op)                                       \n",
    " \n",
    "            ##INITIALIZE SESSION\n",
    "            self._sess = tf.Session()\n",
    " \n",
    "            ##INITIALIZE VARIABLES\n",
    "            init_op = tf.initialize_all_variables()\n",
    "            self._sess.run(init_op)\n",
    " \n",
    "    def _neuron_locations(self, m, n):\n",
    "        \"\"\"\n",
    "        Yields one by one the 2-D locations of the individual neurons\n",
    "        in the SOM.\n",
    "        \"\"\"\n",
    "        #Nested iterations over both dimensions\n",
    "        #to generate all 2-D locations in the map\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                yield np.array([i, j])\n",
    " \n",
    "    def train(self, input_vects):\n",
    "        \"\"\"\n",
    "        Trains the SOM.\n",
    "        'input_vects' should be an iterable of 1-D NumPy arrays with\n",
    "        dimensionality as provided during initialization of this SOM.\n",
    "        Current weightage vectors for all neurons(initially random) are\n",
    "        taken as starting conditions for training.\n",
    "        \"\"\"\n",
    " \n",
    "        #Training iterations\n",
    "        for iter_no in range(self._n_iterations):\n",
    "            #Train with each vector one by one\n",
    "            for input_vect in input_vects:\n",
    "                self._sess.run(self._training_op,\n",
    "                               feed_dict={self._vect_input: input_vect,\n",
    "                                          self._iter_input: iter_no})\n",
    " \n",
    "        #Store a centroid grid for easy retrieval later on\n",
    "        centroid_grid = [[] for i in range(self._m)]\n",
    "        self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "        self._locations = list(self._sess.run(self._location_vects))\n",
    "        for i, loc in enumerate(self._locations):\n",
    "            centroid_grid[loc[0]].append(self._weightages[i])\n",
    "        self._centroid_grid = centroid_grid\n",
    " \n",
    "        self._trained = True\n",
    " \n",
    "    def get_centroids(self):\n",
    "        \"\"\"\n",
    "        Returns a list of 'm' lists, with each inner list containing\n",
    "        the 'n' corresponding centroid locations as 1-D NumPy arrays.\n",
    "        \"\"\"\n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    "        return self._centroid_grid\n",
    " \n",
    "    def map_vects(self, input_vects):\n",
    "        \"\"\"\n",
    "        Maps each input vector to the relevant neuron in the SOM\n",
    "        grid.\n",
    "        'input_vects' should be an iterable of 1-D NumPy arrays with\n",
    "        dimensionality as provided during initialization of this SOM.\n",
    "        Returns a list of 1-D NumPy arrays containing (row, column)\n",
    "        info for each input vector(in the same order), corresponding\n",
    "        to mapped neuron.\n",
    "        \"\"\"\n",
    " \n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    " \n",
    "        to_return = []\n",
    "        for vect in input_vects:\n",
    "            min_index = min([i for i in range(len(self._weightages))],\n",
    "                            key=lambda x: np.linalg.norm(vect-\n",
    "                                                         self._weightages[x]))\n",
    "            to_return.append(self._locations[min_index])\n",
    " \n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b02d8acfa3c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#Train a 20x30 SOM with 400 iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSOM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m97\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m909\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0msom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#Get output grid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-64f1aee13c7b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_vects)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 self._sess.run(self._training_op,\n\u001b[0;32m    155\u001b[0m                                feed_dict={self._vect_input: input_vect,\n\u001b[1;32m--> 156\u001b[1;33m                                           self._iter_input: iter_no})\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;31m#Store a centroid grid for easy retrieval later on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#For plotting the images\n",
    "from matplotlib import pyplot as plt\n",
    " \n",
    "#Training inputs for RGBcolors\n",
    "# colors = np.int64(np.array(onehot_4gram))\n",
    "colors = np.array(d)\n",
    "color_names = n\n",
    "\n",
    "#Train a 20x30 SOM with 400 iterations\n",
    "som = SOM(20, 97, 909, 5000)\n",
    "som.train(colors)\n",
    " \n",
    "#Get output grid\n",
    "image_grid = som.get_centroids()\n",
    " \n",
    "#Map colours to their closest neurons\n",
    "mapped = som.map_vects(colors)\n",
    " \n",
    "#Plot\n",
    "plt.imshow(image_grid)\n",
    "plt.title('Color SOM')\n",
    "for i, m in enumerate(mapped):\n",
    "    plt.text(m[1], m[0], color_names[i], ha='center', va='center',\n",
    "             bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
