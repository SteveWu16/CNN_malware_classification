{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PAD': ('Padding', 0, 0.0, 0, 0.0)}\n",
      "C:/Users/admin/Desktop/DATASET/family/allaple_woj_g_98_year2017/\n",
      "C:/Users/admin/Desktop/DATASET/family/bettersurf_woj_g_137+/\n",
      "C:/Users/admin/Desktop/DATASET/family/elkern_woj_g_127/\n",
      "C:/Users/admin/Desktop/DATASET/family/graftor_g_18/\n",
      "C:/Users/admin/Desktop/DATASET/family/hotbar_g_32/\n",
      "C:/Users/admin/Desktop/DATASET/family/kryptik_g_529/\n",
      "C:/Users/admin/Desktop/DATASET/family/kryptik_g_547/\n",
      "C:/Users/admin/Desktop/DATASET/family/loadmoney_g_183/\n",
      "C:/Users/admin/Desktop/DATASET/family/loring_g_15/\n",
      "C:/Users/admin/Desktop/DATASET/family/mydoom_g_13/\n",
      "C:/Users/admin/Desktop/DATASET/family/rahack_g_39/\n",
      "C:/Users/admin/Desktop/DATASET/family/sytro_woj_g_166/\n",
      "C:/Users/admin/Desktop/DATASET/family/vobfus_g_111/\n",
      "C:/Users/admin/Desktop/DATASET/family/zbot_g_37/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%run C:/Users/admin/Dropbox/Code/example/Data_input.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1751, 128)\n",
      "(1751, 14)\n",
      "(189, 128)\n",
      "(189, 14)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.ndarray(shape = (len(xx_train_list), img_rows*img_cols), buffer = np.array(xx_train_list))\n",
    "y_train = np.array(yy_train_list)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "#x_test = x_train.copy()\n",
    "#y_test = y_train.copy()\n",
    "\n",
    "x_test= np.ndarray(shape = (len(xx_test_list), img_rows*img_cols), buffer = np.array(xx_test_list))\n",
    "y_test = np.array(yy_test_list)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensor_forest.python import tensor_forest\n",
    "from tensorflow.python.ops import resources\n",
    "\n",
    "# Ignore all GPUs, tf random forest does not benefit from it.\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None indicates that the first dimension, corresponding to the batch size, can be of any size.\n",
    "# Parameters\n",
    "num_steps = 5000 # Total steps to train\n",
    "batch_size = 24 # The number of samples per batch\n",
    "num_classes = 14 # The 10 digits\n",
    "num_features = 128 # Each image is 28x28 pixels\n",
    "num_trees = 14\n",
    "max_nodes = 100000\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_rows*img_cols])\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 128\n",
    "n_input = x_train.shape[1]\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "\n",
    "# 測試：for example 64個 0 or 試著用convolution的想法去篩選\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1]), dtype=tf.float32)#,\n",
    "#     'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]), dtype=tf.float32)\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1]), dtype=tf.float32)#,\n",
    "#     'out': tf.Variable(tf.random_normal([n_classes]), dtype=tf.float32)\n",
    "}\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "# train\n",
    "layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "\n",
    "layer_1_sigmoid = tf.nn.sigmoid(layer_1)\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "# 小於threshold的會變成 0\n",
    "NN_threshold = tf.to_int32(layer_1_sigmoid >= threshold) \n",
    "\n",
    "# 轉換data type\n",
    "NN_threshold = tf.cast(NN_threshold, tf.float32)\n",
    "\n",
    "# newx = tf.matmul(x, NN_threshold)\n",
    "newx = x* NN_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_conv(NN_threshold):\n",
    "    # shift\n",
    "    temp1 = NN_threshold[:, 1:] # (?, 127)\n",
    "    \n",
    "    # addition\n",
    "        \n",
    "    zero_temp = tf.constant(0, shape=[batch_size ,1])\n",
    "\n",
    "    zero_temp = tf.cast(zero_temp, tf.float32)\n",
    "\n",
    "    for i in range(x_train.shape[0]):\n",
    "\n",
    "        temp2 = tf.concat([temp1, zero_temp], 1)\n",
    "\n",
    "    output = NN_threshold + temp2\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_addition = custom_conv(NN_threshold)\n",
    "\n",
    "filter_addition = tf.reshape(filter_addition, [-1, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_next_batch(x_test, y_test, seq, start, batch_size):\n",
    "    end = start + batch_size\n",
    "    if end > len(x_test):\n",
    "        # Shuffle the data\n",
    "        perm = np.arange(len(x_test))\n",
    "        np.random.shuffle(perm)\n",
    "        # Start next epoch\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        seq = perm\n",
    "    return x_test[seq][start:end], y_test[seq][start:end], seq, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Constructing forest with params = \n",
      "INFO:tensorflow:{'num_trees': 14, 'max_nodes': 1000, 'bagging_fraction': 1.0, 'feature_bagging_fraction': 1.0, 'num_splits_to_consider': 11, 'max_fertile_nodes': 0, 'split_after_samples': 250, 'valid_leaf_threshold': 1, 'dominate_method': 'bootstrap', 'dominate_fraction': 0.99, 'model_name': 'all_dense', 'split_finish_name': 'basic', 'split_pruning_name': 'none', 'collate_examples': False, 'checkpoint_stats': False, 'use_running_stats_method': False, 'initialize_average_splits': False, 'inference_tree_paths': False, 'param_file': None, 'split_name': 'less_or_equal', 'early_finish_check_every_samples': 0, 'prune_every_samples': 0, 'num_classes': 14, 'num_features': 128, 'bagged_num_features': 128, 'bagged_features': None, 'regression': False, 'num_outputs': 1, 'num_output_columns': 15, 'base_random_seed': 0, 'leaf_model_type': 0, 'stats_model_type': 0, 'finish_type': 0, 'pruning_type': 0, 'split_type': 0}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Parameters\n",
    "hparams = tensor_forest.ForestHParams(num_classes=num_classes,\n",
    "                                      num_features=num_features,\n",
    "                                      num_trees=num_trees,\n",
    "                                      max_nodes=max_nodes).fill()\n",
    "\n",
    "# Build the Random Forest\n",
    "forest_graph = tensor_forest.RandomForestGraphs(hparams)\n",
    "# Get training graph and loss\n",
    "train_op = forest_graph.training_graph(newx, y)\n",
    "loss_op = forest_graph.training_loss(newx, y)\n",
    "\n",
    "# Measure the accuracy\n",
    "infer_op1, infer_op2, infer_op3 = forest_graph.inference_graph(newx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(infer_op1, 1), tf.argmax(y, 1))\n",
    "\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value) and forest resources\n",
    "init_vars = tf.group(tf.global_variables_initializer(), resources.initialize_resources(resources.shared_resources()))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# # Run the initializer\n",
    "sess.run(init_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss: -1.000000, Acc: 1.000000\n",
      "Step 50, Loss: -6.285714, Acc: 0.000000\n",
      "Step 100, Loss: -14.571428, Acc: 0.416667\n",
      "Step 150, Loss: -23.000000, Acc: 0.208333\n",
      "Step 200, Loss: -31.714285, Acc: 0.250000\n",
      "Step 250, Loss: -39.000000, Acc: 0.416667\n",
      "Step 300, Loss: -45.714287, Acc: 0.291667\n",
      "Step 350, Loss: -53.285713, Acc: 0.250000\n",
      "Step 400, Loss: -60.714287, Acc: 0.208333\n",
      "Step 450, Loss: -68.000000, Acc: 0.208333\n",
      "Step 500, Loss: -75.285713, Acc: 0.333333\n",
      "Step 550, Loss: -81.571426, Acc: 0.375000\n",
      "Step 600, Loss: -88.285713, Acc: 0.208333\n",
      "Step 650, Loss: -94.285713, Acc: 0.250000\n",
      "Step 700, Loss: -100.714287, Acc: 0.250000\n",
      "Step 750, Loss: -107.285713, Acc: 0.458333\n",
      "Step 800, Loss: -113.285713, Acc: 0.541667\n",
      "Step 850, Loss: -118.714287, Acc: 0.500000\n",
      "Step 900, Loss: -123.857140, Acc: 0.250000\n",
      "Step 950, Loss: -129.571426, Acc: 0.458333\n",
      "Step 1000, Loss: -135.428574, Acc: 0.250000\n",
      "Step 1050, Loss: -141.571426, Acc: 0.291667\n",
      "Step 1100, Loss: -145.571426, Acc: 0.125000\n",
      "Step 1150, Loss: -151.857147, Acc: 0.583333\n",
      "Step 1200, Loss: -157.000000, Acc: 0.291667\n",
      "Step 1250, Loss: -162.571426, Acc: 0.458333\n",
      "Step 1300, Loss: -168.000000, Acc: 0.416667\n",
      "Step 1350, Loss: -171.857147, Acc: 0.250000\n",
      "Step 1400, Loss: -177.571426, Acc: 0.208333\n",
      "Step 1450, Loss: -182.857147, Acc: 0.458333\n",
      "Step 1500, Loss: -187.000000, Acc: 0.541667\n",
      "Step 1550, Loss: -193.000000, Acc: 0.416667\n",
      "Step 1600, Loss: -197.000000, Acc: 0.208333\n",
      "Step 1650, Loss: -203.857147, Acc: 0.333333\n",
      "Step 1700, Loss: -208.714279, Acc: 0.375000\n",
      "Step 1750, Loss: -213.285721, Acc: 0.250000\n",
      "Step 1800, Loss: -217.571426, Acc: 0.333333\n",
      "Step 1850, Loss: -221.285721, Acc: 0.291667\n",
      "Step 1900, Loss: -227.428574, Acc: 0.375000\n",
      "Step 1950, Loss: -232.000000, Acc: 0.291667\n",
      "Step 2000, Loss: -236.000000, Acc: 0.250000\n",
      "Step 2050, Loss: -240.428574, Acc: 0.041667\n",
      "Step 2100, Loss: -245.714279, Acc: 0.208333\n",
      "Step 2150, Loss: -250.857147, Acc: 0.333333\n",
      "Step 2200, Loss: -254.428574, Acc: 0.125000\n",
      "Step 2250, Loss: -256.857147, Acc: 0.375000\n",
      "Step 2300, Loss: -261.285706, Acc: 0.208333\n",
      "Step 2350, Loss: -265.285706, Acc: 0.416667\n",
      "Step 2400, Loss: -269.571442, Acc: 0.333333\n",
      "Step 2450, Loss: -273.714294, Acc: 0.375000\n",
      "Step 2500, Loss: -278.714294, Acc: 0.291667\n",
      "Step 2550, Loss: -282.714294, Acc: 0.291667\n",
      "Step 2600, Loss: -286.428558, Acc: 0.416667\n",
      "Step 2650, Loss: -290.571442, Acc: 0.541667\n",
      "Step 2700, Loss: -294.857147, Acc: 0.208333\n",
      "Step 2750, Loss: -298.571442, Acc: 0.291667\n",
      "Step 2800, Loss: -304.571442, Acc: 0.291667\n",
      "Step 2850, Loss: -309.571442, Acc: 0.333333\n",
      "Step 2900, Loss: -313.714294, Acc: 0.500000\n",
      "Step 2950, Loss: -317.000000, Acc: 0.166667\n",
      "Step 3000, Loss: -320.142853, Acc: 0.291667\n",
      "Step 3050, Loss: -323.000000, Acc: 0.083333\n",
      "Step 3100, Loss: -326.571442, Acc: 0.250000\n",
      "Step 3150, Loss: -330.285706, Acc: 0.375000\n",
      "Step 3200, Loss: -333.000000, Acc: 0.333333\n",
      "Step 3250, Loss: -337.285706, Acc: 0.333333\n",
      "Step 3300, Loss: -341.857147, Acc: 0.250000\n",
      "Step 3350, Loss: -346.428558, Acc: 0.416667\n",
      "Step 3400, Loss: -350.285706, Acc: 0.500000\n",
      "Step 3450, Loss: -355.000000, Acc: 0.291667\n",
      "Step 3500, Loss: -357.571442, Acc: 0.291667\n",
      "Step 3550, Loss: -360.428558, Acc: 0.416667\n",
      "Step 3600, Loss: -363.000000, Acc: 0.291667\n",
      "Step 3650, Loss: -366.571442, Acc: 0.333333\n",
      "Step 3700, Loss: -370.714294, Acc: 0.333333\n",
      "Step 3750, Loss: -373.428558, Acc: 0.375000\n",
      "Step 3800, Loss: -377.285706, Acc: 0.250000\n",
      "Step 3850, Loss: -380.142853, Acc: 0.208333\n",
      "Step 3900, Loss: -383.428558, Acc: 0.291667\n",
      "Step 3950, Loss: -387.000000, Acc: 0.208333\n",
      "Step 4000, Loss: -391.142853, Acc: 0.333333\n",
      "Step 4050, Loss: -393.714294, Acc: 0.250000\n",
      "Step 4100, Loss: -396.857147, Acc: 0.375000\n",
      "Step 4150, Loss: -400.428558, Acc: 0.291667\n",
      "Step 4200, Loss: -403.571442, Acc: 0.416667\n",
      "Step 4250, Loss: -406.857147, Acc: 0.375000\n",
      "Step 4300, Loss: -410.428558, Acc: 0.416667\n",
      "Step 4350, Loss: -413.428558, Acc: 0.208333\n",
      "Step 4400, Loss: -416.000000, Acc: 0.291667\n",
      "Step 4450, Loss: -419.571442, Acc: 0.416667\n",
      "Step 4500, Loss: -422.000000, Acc: 0.333333\n",
      "Step 4550, Loss: -425.714294, Acc: 0.333333\n",
      "Step 4600, Loss: -428.857147, Acc: 0.291667\n",
      "Step 4650, Loss: -430.714294, Acc: 0.166667\n",
      "Step 4700, Loss: -434.285706, Acc: 0.250000\n",
      "Step 4750, Loss: -437.714294, Acc: 0.291667\n",
      "Step 4800, Loss: -441.000000, Acc: 0.208333\n",
      "Step 4850, Loss: -444.714294, Acc: 0.375000\n",
      "Step 4900, Loss: -448.571442, Acc: 0.375000\n",
      "Step 4950, Loss: -452.142853, Acc: 0.250000\n",
      "Step 5000, Loss: -455.000000, Acc: 0.250000\n",
      "Test Accuracy: 0.3227513\n"
     ]
    }
   ],
   "source": [
    "b = 0\n",
    "seq = np.arange(len(x_train))\n",
    "\n",
    "# Training\n",
    "for i in range(1, num_steps + 1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    batch_x, batch_y, seq, b = my_next_batch(x_train, y_train, seq, b, batch_size)\n",
    "    \n",
    "    _, l = sess.run([train_op, loss_op], feed_dict={x: batch_x, y: batch_y})\n",
    "    \n",
    "    if i % 50 == 0 or i == 1:\n",
    "        acc = accuracy_op.eval(feed_dict={x: batch_x, y: batch_y})\n",
    "        print('Step %i, Loss: %f, Acc: %f' % (i, l, acc))\n",
    "\n",
    "# Test Model\n",
    "print(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={x: x_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
