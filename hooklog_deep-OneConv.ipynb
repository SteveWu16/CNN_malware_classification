{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hooklog DEEP\n",
    "from https://www.tensorflow.org/get_started/mnist/pros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_rate = 0.1\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 1, 128\n",
    "filter_x, filter_y = 1, 16\n",
    "\n",
    "out_channel = 128\n",
    "\n",
    "pool_size = 4\n",
    "\n",
    "API_padding = False\n",
    "API_padding = True # with API padding and without API moving window\n",
    "API_moving = API_padding = True # with API padding and API moving window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run C:/Users/admin/Dropbox/Code/example/Hooklog3.ipynb\n",
    "Hooklog = Hooklog3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 ['allaple_woj_g_98_year2017', 'bettersurf_woj_g_137+', 'elkern_woj_g_127', 'expiro_g_56', 'hotbar_g_32', 'kryptik_g_529', 'kryptik_g_547', 'loadmoney_g_184', 'lydra_woj_g_44', 'mira_woj_g_107', 'morstar_g_54', 'mydoom_g_13', 'rahack_g_39', 'somoto_woj_year2017', 'sytro_woj_g_166', 'vobfus_g_111', 'zbot_g_37', 'zusy_g_127']\n"
     ]
    }
   ],
   "source": [
    "in_directories = [\"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/allaple_woj_g_98_year2017/\", \n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/bettersurf_woj_g_137+/\", \n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/elkern_woj_g_127/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/expiro_g_56/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/hotbar_g_32/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/kryptik_g_529/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/kryptik_g_547/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/loadmoney_g_184/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/lydra_woj_g_44/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/mira_woj_g_107/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/morstar_g_54/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/mydoom_g_13/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/rahack_g_39/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/somoto_woj_year2017/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/sytro_woj_g_166/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/vobfus_g_111/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/zbot_g_37/\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/hooklogs_goodfamily/zusy_g_127/\",\n",
    "                 ]\n",
    "in_parseFirstPar = False\n",
    "\n",
    "in_apifreq_dicts = [\"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_allaple_woj_g_98_year2017.pickle\", \n",
    "                   \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_bettersurf_woj_g_137+.pickle\", \n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_elkern_woj_g_127.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_expiro_g_56.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_hotbar_g_32.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_kryptik_g_529.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_kryptik_g_547.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_loadmoney_g_184.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_lydra_woj_g_44.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_mira_woj_g_107.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_morstar_g_54.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_mydoom_g_13.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_rahack_g_39.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_somoto_woj_year2017.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_sytro_woj_g_166.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_vobfus_g_111.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_zbot_g_37.pickle\",\n",
    "                  \"C:/Users/admin/Dropbox/Code/example/good_pickle/hl_list_zusy_g_127.pickle\",\n",
    "                 ]\n",
    "num_classes = len(in_directories)\n",
    "classnames = list(map(lambda x: x.split(\"/\")[-2], in_directories))\n",
    "print(num_classes, classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'InternetConnect': ('InternetConnect', 0, 0.0, 74, 0.0006073289835446674), 'HttpSendRequest': ('HttpSendRequest', 1, 0.0625, 74, 0.0006073289835446674), 'InternetOpen': ('InternetOpen', 2, 0.125, 75, 0.0006155361319709467), 'DeleteFile': ('DeleteFile', 3, 0.1875, 155, 0.0012721080060732898), 'CreateProcess': ('CreateProcess', 4, 0.25, 160, 0.0013131437482046863), 'ExitProcess': ('ExitProcess', 5, 0.3125, 162, 0.0013295580450572448), 'CreateProcessInternal': ('CreateProcessInternal', 6, 0.375, 251, 0.002059994254996102), 'CreateThread': ('CreateThread', 7, 0.4375, 364, 0.0029874020271656613), 'RegDeleteKey': ('RegDeleteKey', 8, 0.5, 518, 0.004251302884812672), 'OpenProcess': ('OpenProcess', 9, 0.5625, 1210, 0.00993064959579794), 'RegSetValue': ('RegSetValue', 10, 0.625, 3515, 0.0288481267183717), 'RegCreateKey': ('RegCreateKey', 11, 0.6875, 4047, 0.03321432968115228), 'CopyFile': ('CopyFile', 12, 0.75, 6771, 0.05557060199433707), 'RegEnumValue': ('RegEnumValue', 13, 0.8125, 8353, 0.0685543108047109), 'LoadLibrary': ('LoadLibrary', 14, 0.875, 10376, 0.0851573720710739), 'CreateFile': ('CreateFile', 15, 0.9375, 34664, 0.2844925930485453), 'RegQueryValue': ('RegQueryValue', 16, 1.0, 51076, 0.41918831302064097)}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "apifreq_dict = dict()\n",
    "_total = 0\n",
    "for pkf in in_apifreq_dicts:\n",
    "    with open(pkf, 'rb') as f:\n",
    "        this_dict = pickle.load(f)\n",
    "        for k in this_dict:\n",
    "            if k in apifreq_dict:\n",
    "                apifreq_dict[k] += this_dict[k]\n",
    "            else:\n",
    "                apifreq_dict[k] = this_dict[k]\n",
    "            _total += this_dict[k]\n",
    "\n",
    "s_dict = {item[0]: item for item in [(k, i, i/(len(apifreq_dict)-1), apifreq_dict[k], apifreq_dict[k]/_total) for i, k in enumerate(sorted(apifreq_dict, key = apifreq_dict.get, reverse=False))]}\n",
    "print(s_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "W_Size = 128 # moving window size\n",
    "\n",
    "xx_train_list = list()\n",
    "yy_train_list = list()\n",
    "\n",
    "xx_test_list = list()\n",
    "yy_test_list = list()\n",
    "\n",
    "train_name_list = list()\n",
    "test_name_list = list()\n",
    "\n",
    "name_dict = dict()\n",
    "count_no = 0\n",
    "for label, in_dir in enumerate(in_directories):\n",
    "    hl_list = next(os.walk(in_dir))[2] # get all filenames in the in_directory\n",
    "    hl_list = [os.path.join(in_dir, f) for f in hl_list] # filepathname list\n",
    "    hl_list = list(filter(lambda f: f.endswith(\".hooklog\"), hl_list)) # in case some non-hooklog file in the folder\n",
    "    \n",
    "    #shuffle list\n",
    "    random.shuffle(hl_list) # <-------------- random here\n",
    "    test_size = int(len(hl_list)*test_rate) # test rate = 0.1\n",
    "\n",
    "    \n",
    "    for i, file in enumerate(hl_list):\n",
    "        hl3 = Hooklog(file, in_parseFirstPar)\n",
    "        li_li = list() # for hacking moving_window # 20171129\n",
    "        \n",
    "        count_no += 1\n",
    "        \n",
    "        for start in range(0, len(hl3.li), W_Size): # img_cols = 128 0 to length of hooklog\n",
    "            end = start + img_cols\n",
    "#             print(hl3.digitname, start, end)\n",
    "            \n",
    "            li = list()\n",
    "            for (t, api) in hl3.li[start:end]: # moving\n",
    "                li.append(s_dict[api][4]) # <-- encode\n",
    "            \n",
    "            # hack\n",
    "            if len(li) < img_cols:\n",
    "                #print(\"!!! change img_cols to a smaller number\", len(hl3.li))\n",
    "                #print(hl3.digitname, \"has smaller size\", len(hl3.li), \"need img_cols\", str(img_cols))\n",
    "                if API_padding:\n",
    "                    #print(\"padding 1.0\", str(img_cols - len(hl3.li)))\n",
    "                    for _ in range(img_cols - len(li)):\n",
    "                        li.append(0)\n",
    "                        #li.append(1.0)\n",
    "            li_li.append(li)\n",
    "            \n",
    "#             print('start')\n",
    "#             print(count_no)\n",
    "#             print(len(li))\n",
    "#             print(li)\n",
    "#             count_no += 1\n",
    "#             print('end')\n",
    "            \n",
    "            if(i < test_size):\n",
    "                xx_test_list.extend([li])\n",
    "                a = [0] * num_classes\n",
    "                a[label] = 1\n",
    "                yy_test_list.extend([a])\n",
    "                test_name_list.append((hl3.digitname, start))\n",
    "                name_dict[hl3.digitname] = label\n",
    "            else:\n",
    "                xx_train_list.extend([li])\n",
    "                a = [0] * num_classes\n",
    "                a[label] = 1\n",
    "                yy_train_list.extend([a])\n",
    "                train_name_list.append((hl3.digitname, start))\n",
    "                name_dict[hl3.digitname] = label\n",
    "\n",
    "print(count_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1044, 128)\n",
      "(1044, 18)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#x_train = np.ndarray(shape = (len(xx_train_list), img_rows, img_cols), buffer = np.array(xx_train_list))\n",
    "#y_train = np.array(yy_train_list)\n",
    "x_train = np.ndarray(shape = (len(xx_train_list), img_rows*img_cols), buffer = np.array(xx_train_list))\n",
    "y_train = np.array(yy_train_list)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 128)\n",
      "(90, 18)\n"
     ]
    }
   ],
   "source": [
    "#x_test = x_train.copy()\n",
    "#y_test = y_train.copy()\n",
    "\n",
    "x_test= np.ndarray(shape = (len(xx_test_list), img_rows*img_cols), buffer = np.array(xx_test_list))\n",
    "y_test = np.array(yy_test_list)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(train_name_list)\n",
    "#print(test_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_rows*img_cols]) # None, 784\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, num_classes]) # None, 8\n",
    "# None indicates that the first dimension, corresponding to the batch size, can be of any size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer\n",
    "https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_convolutional_neural_networks_work.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') # \"SAME\" tries to pad evenly left and right\n",
    "    #The stride of the sliding window for each dimension of input x. \n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, pool_size, 1], strides=[1, 1, 4, 1], padding='SAME') # [1, 2, 2, 1], [1, 2, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First Convolutional Layer\n",
    "\n",
    "# [1,4,1,32]\n",
    "W_conv1 = weight_variable([filter_x, filter_y, 1, out_channel]) # [filter_height, filter_width, in_channels, out_channels] [5, 5, 1, 32]\n",
    "b_conv1 = bias_variable([out_channel]) # 32\n",
    "\n",
    "x_image = tf.reshape(x, [-1, img_rows, img_cols, 1]) # [batch, in_height, in_width, in_channels] [-1, 28, 28, 1]\n",
    "\n",
    "output_conv1 = conv2d(x_image, W_conv1) + b_conv1 ###\n",
    "h_conv1 = tf.nn.relu(output_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Second Convolutional Layer\n",
    "# W_conv2 = weight_variable([filter_x, filter_y, 32, 64]) # [5, 5, 32, 64]\n",
    "# b_conv2 = bias_variable([64]) # 64\n",
    "\n",
    "# output_conv2 = conv2d(h_pool1, W_conv2) + b_conv2 ###\n",
    "# h_conv2 = tf.nn.relu(output_conv2)\n",
    "# h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Densely Connected Layer\n",
    "s = int(img_rows*img_cols/4) # hack here, 2d 2x2 2 layers = 1d 1x4 2 layers\n",
    "\n",
    "W_fc1 = weight_variable([1 * s * out_channel, out_channel*2]) # [7 * 7 * 64, 1024]\n",
    "b_fc1 = bias_variable([out_channel*2]) # 1024\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool1, [-1, 1*s*out_channel]) # [-1, 7*7*64] ###\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Readout Layer\n",
    "W_fc2 = weight_variable([out_channel*2, num_classes]) # [1024, 8]\n",
    "b_fc2 = bias_variable([num_classes]) # 8\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and Evaluate \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 1000 90 50\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "training_epochs = 1000\n",
    "testing_size = len(x_test)\n",
    "print_interval = 50\n",
    "\n",
    "print(batch_size, training_epochs, testing_size, print_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# use this one to keep the session\\nsess = tf.InteractiveSession()\\nsess.run(tf.global_variables_initializer())\\nfor i in range(training_epochs):\\n    batch = mnist.train.next_batch(50)\\n    if i%print_interval == 0:\\n        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\\n        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\\n    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# use this one to keep the session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(training_epochs):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%print_interval == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_next_batch(x_test, y_test, seq, start, batch_size):\n",
    "    end = start + batch_size\n",
    "    if end > len(x_test):\n",
    "        # Shuffle the data\n",
    "        perm = np.arange(len(x_test))\n",
    "        np.random.shuffle(perm)\n",
    "        # Start next epoch\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        seq = perm\n",
    "    return x_test[seq][start:end], y_test[seq][start:end], seq, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0\n",
      "step 50, training accuracy 0\n",
      "step 100, training accuracy 0.6\n",
      "step 150, training accuracy 0.45\n",
      "step 200, training accuracy 0.3\n",
      "step 250, training accuracy 0.55\n",
      "step 300, training accuracy 0.5\n",
      "step 350, training accuracy 0.5\n",
      "step 400, training accuracy 0.5\n",
      "step 450, training accuracy 0.85\n",
      "step 500, training accuracy 0.6\n",
      "step 550, training accuracy 0.55\n",
      "step 600, training accuracy 0.7\n",
      "step 650, training accuracy 0.65\n",
      "step 700, training accuracy 0.45\n",
      "step 750, training accuracy 0.9\n",
      "step 800, training accuracy 0.6\n",
      "step 850, training accuracy 0.75\n",
      "step 900, training accuracy 0.75\n",
      "step 950, training accuracy 0.55\n",
      "Cost 12.754912 sec\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "import time\n",
    "tStart = time.time()\n",
    "\n",
    "b = 0\n",
    "seq = np.arange(len(x_train))\n",
    "for i in range(training_epochs):\n",
    "    batch_x, batch_y, seq, b = my_next_batch(x_train, y_train, seq, b, batch_size)\n",
    "    \n",
    "    if i%print_interval == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch_x, y_: batch_y, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.5})\n",
    "\n",
    "tEnd = time.time()\n",
    "tTime = tEnd - tStart\n",
    "print(\"Cost %f\" %(tTime), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.877778\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: x_test[:testing_size], y_: y_test[:testing_size], keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_layer(layer, image):\n",
    "    # layer 輸入範例是 [batch_num, width, height, channels] ex. [1, 28, 28, 32]\n",
    "    # 這裡設定的 image 是單一個手寫字影像輸入\n",
    "    # 接下來把它餵入 tensorflow 的 session 跑出我們想要的輸出結果\n",
    "#     sess = tf.InteractiveSession()\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    output = sess.run(layer, feed_dict = {x: [image]}) # 執行 Convolution / Pooling\n",
    "    \n",
    "    print('output.shape = ' + str(output.shape))\n",
    "    print(type(output))\n",
    "    print(output[0,0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_fc_layer(layer, image):\n",
    "#     sess = tf.InteractiveSession()\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    output = sess.run(layer, feed_dict = {x: [image]})\n",
    "    \n",
    "    fc = tf.nn.relu(tf.matmul(output, W_fc1) + b_fc1)\n",
    "    \n",
    "    sfc = sess.run(tf.reshape(fc,[1024]))\n",
    "    \n",
    "    print('output.shape = ' + str(fc.shape))\n",
    "    print(type(fc))\n",
    "    print(sfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_ro_layer(layer, image):\n",
    "#     sess = tf.InteractiveSession()\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    output = sess.run(layer, feed_dict = {x: [image]})\n",
    "    \n",
    "#     h_fc1 = tf.nn.relu(tf.matmul(output, W_fc1) + b_fc1)\n",
    "    \n",
    "#     keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    print('output.shape = ' + str(y_conv.shape))\n",
    "    print(type(y_conv))\n",
    "    print(sess.run(y_conv, feed_dict = {x: [image], keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 128)\n"
     ]
    }
   ],
   "source": [
    "print(batch_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.41918831  0.41918831  0.41918831  0.41918831  0.08515737  0.41918831\n",
      "  0.08515737  0.28449259  0.08515737  0.41918831  0.08515737  0.03321433\n",
      "  0.41918831  0.08515737  0.41918831  0.41918831  0.41918831  0.41918831\n",
      "  0.41918831  0.41918831  0.41918831  0.41918831  0.41918831  0.41918831\n",
      "  0.41918831  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431  0.06855431  0.41918831  0.41918831  0.41918831\n",
      "  0.41918831  0.28449259  0.28449259  0.08515737  0.28449259  0.41918831\n",
      "  0.03321433  0.41918831  0.28449259  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431  0.06855431  0.03321433  0.02884813  0.03321433\n",
      "  0.41918831  0.41918831  0.41918831  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431  0.06855431  0.06855431  0.06855431  0.41918831\n",
      "  0.41918831  0.41918831  0.41918831  0.28449259  0.28449259  0.41918831\n",
      "  0.03321433  0.41918831  0.28449259  0.06855431  0.06855431  0.06855431\n",
      "  0.06855431  0.06855431]\n",
      "(128,)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(batch_x[0]) # 128\n",
    "print(batch_x[0].shape)\n",
    "print(type(batch_x[0]))\n",
    "print(batch_y[0]) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941]\n",
      "(64,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "c_in = [0.07609941330414108 for i in range(64)]\n",
    "c_input = np.asarray(c_in)\n",
    "print(c_input)\n",
    "print(c_input.shape)\n",
    "print(type(c_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941  0.07609941\n",
      "  0.07609941  0.07609941  0.07609941  0.07609941]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 64) for Tensor 'Placeholder_3:0', which has shape '(?, 128)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-d3a838dcf53c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimage1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplot_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_conv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_conv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-ae56ed788c6e>\u001b[0m in \u001b[0;36mplot_layer\u001b[1;34m(layer, image)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#     sess.run(tf.global_variables_initializer())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 執行 Convolution / Pooling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output.shape = '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TensorFlow_GPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\TensorFlow_GPU\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    959\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    962\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 64) for Tensor 'Placeholder_3:0', which has shape '(?, 128)'"
     ]
    }
   ],
   "source": [
    "output_conv1 = conv2d(x_image, W_conv1) + b_conv1\n",
    "image1 = batch_x[0] # input dat\n",
    "image1 = c_input\n",
    "print(image1)\n",
    "plot_layer(h_conv1, image1)\n",
    "\n",
    "print(type(h_conv1))\n",
    "\n",
    "print(h_conv1[0,0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pool1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "plot_layer(h_pool1, image1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight\n",
    "## conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(W_conv1)\n",
    "a = sess.run(W_conv1)\n",
    "for i in range(32):\n",
    "    print(a[0,:,0,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(b_conv1)\n",
    "print(b_conv1.shape)\n",
    "a = sess.run(b_conv1)\n",
    "for i in range(32):\n",
    "    print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
